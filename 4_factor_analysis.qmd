---
title: "Factor analysis"
format: 
  html:
    toc: true
execute: 
  cache: true
---

```{r}
library(tidyverse)
library(lavaan)
library(tidySEM)
library(here)
library(psych)

d_sub <- readRDS(here("cached_intermediates","1_d_sub.Rds"))
d_trial <- readRDS(here("cached_intermediates","1_d_trial.Rds"))

```

Our goal here is to understand the dimensionality of the data. 

Let's use the approach advocated by Nilam, namely test for a single dimension of latent variation. 

# Full dataset

```{r}
d_sub_mat <- d_sub |>
  ungroup() |>
  select(dataset_name, rt, rt_var, long_window_accuracy, long_window_acc_var, prod, comp) 

d_sub_mat_s <- d_sub_mat |>
  ungroup() |>
  mutate(across(all_of(c("rt", "rt_var", "long_window_accuracy", 
                         "long_window_acc_var", "prod", "comp")), 
                       ~ scale(.x)[,1])) 
```
         
## EFA 

First we will fit EFA. You can do this over unscaled data, result is the same. 

```{r}
fa.parallel(select(d_sub_mat, -dataset_name), fa = "fa", 
            use = "pairwise.complete.obs")
```

There is some variation on the second dimension, though.

```{r}
fa(select(d_sub_mat, -dataset_name), nfactor = 2,
            use = "pairwise", rotate = "varimax")
```


## CFA 

We start with a one factor CFA.

```{r}
fa_model <-  "F1  =~ rt + rt_var + long_window_accuracy + long_window_acc_var + prod + comp"
fit <- cfa(fa_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit, fit.measures=TRUE, standardize=TRUE)
```
r2:

```{r}
inspect(fit, 'r2')
```

Missing data coverage: 

```{r}
inspect(fit, 'coverage')
```
Look at observed vs. fitted correlations. 

```{r}
inspect(fit, what="cor.all")
lavCor(fit)
# resid(fit, "cor")

```


```{r}
graph_sem(model = fit, text_size = 2) + 
  theme(panel.background = element_rect(fill = "white"))
```

Extract F1. 

```{r}
d_sub$f1 <- lavPredict(fit, newdata = d_sub_mat_s)
ggplot(d_sub, aes(x = log_age, y = f1)) + 
  geom_point(alpha = .1) + 
  geom_smooth() + 
  geom_smooth(method = "lm", col = "green")
```

This seems somewhat satisfactory (all loadings are in the same direction). But to be honest, the fit indices are not great. This is probably driven by some of the action in the second dimension in the scree plot. Not sure what is going on there, but part of it has to do with the relative lack of correlation between RT and comprehension. I wonder if that has to do with the relatively small amount of comprehension data we have. 

# Alternative models

## Speed/accuracy separate

Compare to model with separate speed and accuracy. This one is better fitting.

```{r}

fa2_model <-  "accuracy =~ long_window_accuracy + long_window_acc_var + prod + comp
               speed =~ rt + rt_var"

fit2 <- cfa(fa2_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit2, fit.measures=TRUE, standardize=TRUE)
```

```{r}
layout = matrix(nrow=4, ncol = 4, 
                data = c(NA,NA,NA,"prod",
                         "rt_var",NA,NA,"comp",
                         NA,"speed","accuracy","long_window_accuracy",
                         "rt",NA,NA,"long_window_acc_var"), byrow = TRUE)

graph_sem(model = fit2, text_size = 2, layout = layout )

```

Big improvement. 

```{r}
inspect(fit2, 'r2')
```

```{r}
anova(fit, fit2)
```

## Variation separated 

Compare to model with vars separate but acc and RT grouped. This is definitely worse. 

```{r}

fa2v_model <-  "language =~ rt + long_window_accuracy + prod + comp
               variability =~  long_window_acc_var + rt_var"

fit2v <- cfa(fa2v_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit2v, fit.measures=TRUE, standardize=TRUE)
```
```{r}
graph_sem(model = fit2v, text_size = 2) 
```

Model comparison. There is an improvement.

```{r}
anova(fit, fit2v)
```

Which of these is better? 

```{r}
anova(fit2, fit2v)
```

The speed/accuracy model has vastly better fit. 

## CDI vs. LWL

Compare to model with CDI variables separated. Note this is confounded by dataset. 

```{r}

fa2t_model <-  "task =~ rt + long_window_accuracy + long_window_acc_var + rt_var
               CDI =~  prod + comp"

fit2t <- cfa(fa2t_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit2t, fit.measures=TRUE, standardize=TRUE)
```

```{r}
graph_sem(model = fit2t, text_size = 2) 
```

Model comparison. There is an improvement.

```{r}
anova(fit, fit2t)
```

Which of these is better? 

```{r}
anova(fit2, fit2t)
```

## No comprehension

Removing comprehension doesn't seem to clean up the misfit issues. 

```{r}
fa_prod_model <-  "F1  =~ rt + rt_var + long_window_accuracy + long_window_acc_var + prod"
fit_prod <- cfa(fa_prod_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit_prod, fit.measures=TRUE, standardize=TRUE)
```



```{r}
graph_sem(model = fit_prod, text_size = 2)
```




## Without variances

Without variance.

```{r}

fa_model_novar <-  "F1  =~ rt + long_window_accuracy + prod + comp"
fit_novar <- cfa(fa_model_novar, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit_novar, fit.measures = TRUE, standardized =TRUE)
```

```{r}
graph_sem(model = fit_novar) + 
  theme(panel.background = element_rect(fill = "white"))
```

# Three-factor mode

Compare to model with separate speed and accuracy. This one is better fitting.

```{r}

fa3_model <- "vocab =~ prod + comp
              accuracy =~ long_window_accuracy + long_window_acc_var
              speed =~ rt + rt_var"


fit3 <- cfa(fa3_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit3, fit.measures=TRUE, standardize=TRUE)
```

```{r}
layout = matrix(nrow=2, ncol = 6,
                data = c("speed",NA,"accuracy",NA,"vocab",NA,
                         "rt_var","rt", "long_window_accuracy",
                         "long_window_acc_var", "prod","comp"), 
                byrow = TRUE)

graph_sem(model = fit3, text_size = 2, layout = layout)

```


# Fit models with dataset grouping

One big worry here is that it's between-group confounding that leads to model misfit, not actual factor structure. To deal with this, we fit the same model but with grouping by dataset. 

We can't really handle the missing data here sadly, because there's too much missing prod and comp data. 

```{r}
fa_model_reduced <-  "F1  =~ rt + rt_var + long_window_accuracy + long_window_acc_var"

fit_reduced <- cfa(fa_model_reduced, d_sub_mat_s, std.lv=TRUE, 
                   missing='fiml')
summary(fit_reduced, fit.measures=TRUE, standardize=TRUE)
```

Now the grouped dataset. 

```{r}
fit_reduced_grouped <- cfa(fa_model_reduced, d_sub_mat_s, std.lv=TRUE, 
                   missing='fiml', 
                   group = "dataset_name")

summary(fit_reduced_grouped, fit.measures=TRUE, standardize=TRUE)
```

We can look at invariance.

```{r}
fit_reduced_grouped_loadings <- cfa(fa_model_reduced, d_sub_mat_s, std.lv=TRUE, 
                   missing='fiml', 
                   group = "dataset_name", 
                   group.equal = "loadings")

# summary(fit_reduced_grouped_loadings, fit.measures=TRUE, standardize=TRUE)

lavTestLRT(fit_reduced_grouped, fit_reduced_grouped_loadings)
```

This test suggests that we reject the null and the non-invariant model is better. 

## Fernald and Marchman only

```{r}
fm_data <- d_sub_mat_s |>
  filter(dataset_name %in% c("adams_marchman_2018","fmw_2013"))

fa_model <-  "F1  =~ rt + rt_var + long_window_accuracy + long_window_acc_var + prod + comp"

fm_fit <- cfa(fa_model, fm_data, std.lv=TRUE, missing='fiml')

summary(fm_fit, fit.measures=TRUE, standardize=TRUE)
```
```{r}
fa2_model <-  "accuracy =~ long_window_accuracy + long_window_acc_var + prod + comp
               speed =~ rt + rt_var"

fm_fit2 <- cfa(fa2_model, fm_data, std.lv=TRUE, missing='fiml')

summary(fm_fit2, fit.measures=TRUE, standardize=TRUE)
```

