---
title: "Factor analysis"
format: 
  html:
    toc: true
execute: 
  cache: true
---

```{r}
library(tidyverse)
library(lavaan)
library(tidySEM)
library(here)
library(psych)

d_sub <- readRDS(here("cached_intermediates","1_d_sub.Rds"))
d_trial <- readRDS(here("cached_intermediates","1_d_trial.Rds"))

```

Our goal here is to understand the dimensionality of the data. 

Let's use the approach advocated by Nilam, namely test for a single dimension of latent variation. 

# Full dataset

```{r}
d_sub_mat <- d_sub |>
  ungroup() |>
  select(dataset_name, rt, rt_var, long_window_accuracy, long_window_acc_var, prod, comp, age) 


d_sub_mat_s <- d_sub_mat |>
  ungroup() |>
  mutate(across(all_of(c("rt", "rt_var", "long_window_accuracy", 
                         "long_window_acc_var", "prod", "comp")), 
                       ~ age_scale(.x, age))) 
```
         
## EFA 

First we will fit EFA. You can do this over unscaled data, result is the same. 

```{r}
fa.parallel(select(d_sub_mat, -dataset_name, -age), fa = "fa", 
            use = "pairwise.complete.obs")
```

There is some variation on the second dimension, though.

```{r}
fa(select(d_sub_mat, -dataset_name), nfactor = 2,
            use = "pairwise", rotate = "varimax")
```
Here's what this looks like not forced to be orthogonal. 

```{r}
fa(select(d_sub_mat, -dataset_name, -age), nfactor = 2,
            use = "pairwise", rotate = "oblimin")

```

## CFA 

We start with a one factor CFA.

```{r}
fa_model <-  "F1  =~ rt + rt_var + long_window_accuracy + long_window_acc_var + prod + comp"
fit <- cfa(fa_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit, fit.measures=TRUE, standardize=TRUE)
```
r2:

```{r}
inspect(fit, 'r2')
```

Missing data coverage: 

```{r}
inspect(fit, 'coverage')
```
Look at observed vs. fitted correlations. 

```{r}
inspect(fit, what="cor.all")
lavCor(fit)
# resid(fit, "cor")

```


```{r}
graph_sem(model = fit, text_size = 2) + 
  theme(panel.background = element_rect(fill = "white"))
```

Extract F1. 

```{r}
d_sub$f1 <- lavPredict(fit, newdata = d_sub_mat_s)
ggplot(d_sub, aes(x = log_age, y = f1)) + 
  geom_point(alpha = .1) + 
  geom_smooth() + 
  geom_smooth(method = "lm", col = "green")
```

This seems somewhat satisfactory (all loadings are in the same direction). But to be honest, the fit indices are not great. This is probably driven by some of the action in the second dimension in the scree plot. Not sure what is going on there, but part of it has to do with the relative lack of correlation between RT and comprehension. I wonder if that has to do with the relatively small amount of comprehension data we have. 

# Alternative models

## Speed/accuracy separate

Compare to model with separate speed and accuracy. This one is better fitting.

```{r}

fa2_model <-  "accuracy =~ long_window_accuracy + long_window_acc_var + prod + comp
               speed =~ rt + rt_var"

fit2 <- cfa(fa2_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit2, fit.measures=TRUE, standardize=TRUE)
```

```{r}
layout = matrix(nrow=4, ncol = 4, 
                data = c(NA,NA,NA,"prod",
                         "rt_var",NA,NA,"comp",
                         NA,"speed","accuracy","long_window_accuracy",
                         "rt",NA,NA,"long_window_acc_var"), byrow = TRUE)

graph_sem(model = fit2, text_size = 2, layout = layout )

```

Big improvement. 

```{r}
inspect(fit2, 'r2')
```

```{r}
anova(fit, fit2)
```

## Variation separated 

Compare to model with vars separate but acc and RT grouped. This is definitely worse. 

```{r}

fa2v_model <-  "language =~ rt + long_window_accuracy + prod + comp
               variability =~  long_window_acc_var + rt_var"

fit2v <- cfa(fa2v_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit2v, fit.measures=TRUE, standardize=TRUE)
```
```{r}
graph_sem(model = fit2v, text_size = 2) 
```

Model comparison. There is an improvement.

```{r}
anova(fit, fit2v)
```

Which of these is better? 

```{r}
anova(fit2, fit2v)
```

The speed/accuracy model has vastly better fit. 

## CDI vs. LWL

Compare to model with CDI variables separated. Note this is confounded by dataset. 

```{r}

fa2t_model <-  "task =~ rt + long_window_accuracy + long_window_acc_var + rt_var
               CDI =~  prod + comp"

fit2t <- cfa(fa2t_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit2t, fit.measures=TRUE, standardize=TRUE)
```

```{r}
graph_sem(model = fit2t, text_size = 2) 
```

Model comparison. There is an improvement.

```{r}
anova(fit, fit2t)
```

Which of these is better? 

```{r}
anova(fit2, fit2t)
```

## No comprehension

Removing comprehension doesn't seem to clean up the misfit issues. 

```{r}
fa_prod_model <-  "F1  =~ rt + rt_var + long_window_accuracy + long_window_acc_var + prod"
fit_prod <- cfa(fa_prod_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit_prod, fit.measures=TRUE, standardize=TRUE)
```



```{r}
graph_sem(model = fit_prod, text_size = 2)
```




## Without variances

Without variance.

```{r}

fa_model_novar <-  "F1  =~ rt + long_window_accuracy + prod + comp"
fit_novar <- cfa(fa_model_novar, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit_novar, fit.measures = TRUE, standardized =TRUE)
```

```{r}
graph_sem(model = fit_novar) + 
  theme(panel.background = element_rect(fill = "white"))
```

# Three-factor model

Compare to model with separate speed and accuracy. This one is better fitting.

```{r}

fa3_model <- "vocab =~ prod + comp
              accuracy =~ long_window_accuracy + long_window_acc_var
              speed =~ rt + rt_var"


fit3 <- cfa(fa3_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit3, fit.measures=TRUE, standardize=TRUE)
```

```{r}
layout = matrix(nrow=2, ncol = 6,
                data = c("speed",NA,"accuracy",NA,"vocab",NA,
                         "rt_var","rt", "long_window_accuracy",
                         "long_window_acc_var", "prod","comp"), 
                byrow = TRUE)

graph_sem(model = fit3, text_size = 2, layout = layout)

```

## Age regression

```{r}

fa3_age_model <- "
# measurement model
vocab =~ prod + comp
accuracy =~ long_window_accuracy + long_window_acc_var
speed =~ rt + rt_var

# regressions
vocab ~ age
accuracy ~ age
speed ~ age
"

fit3_age <- cfa(fa3_age_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

summary(fit3_age, fit.measures=TRUE, standardize=TRUE)
layout_age = matrix(nrow=5, ncol = 6,
                data = c(NA,NA,"age",NA, NA, NA,
                         NA, NA, NA, NA, NA, NA, 
                         "speed",NA,"accuracy",NA,"vocab",NA,
                         NA, NA, NA, NA, NA, NA, 
                         "rt_var","rt", "long_window_accuracy",
                         "long_window_acc_var", "prod","comp"), 
                byrow = TRUE)

graph_sem(model = fit3_age, text_size = 2, layout = t(layout_age))
```



# Fit models with dataset grouping

One big worry here is that it's between-group confounding that leads to model misfit, not actual factor structure. To deal with this, we fit the same model but with grouping by dataset. 

These mdels generally do not work sadly. 

Now the grouped dataset. 

```{r}
fa3_model_l2 <- "
level: 1
  vocab =~ prod + comp
  accuracy =~ long_window_accuracy + long_window_acc_var
  speed =~ rt + rt_var

level: 2
  vocab =~ prod + comp
  accuracy =~ long_window_accuracy + long_window_acc_var
  speed =~ rt + rt_var"

# optimization from https://lavaan.ugent.be/tutorial/multilevel.html
fit3_grouped <- cfa(fa3_model_l2, d_sub_mat_s, std.lv=TRUE, missing='fiml', 
                    cluster = "dataset_name", 
                    optim.method = "em", 
                    em.iter.max = 20000,
                    em.fx.tol = 1e-08, em.dx.tol = 1e-04)

summary(fit3_grouped, fit.measures=TRUE, standardize=TRUE)
```

We can look at invariance. Not implemented due to convergence issues. 

```{r}
# fit_reduced_grouped_loadings <- cfa(fa_model_reduced, d_sub_mat_s, std.lv=TRUE, 
#                    missing='fiml', 
#                    group = "dataset_name", 
#                    group.equal = "loadings")

# summary(fit_reduced_grouped_loadings, fit.measures=TRUE, standardize=TRUE)

# lavTestLRT(fit_reduced_grouped, fit_reduced_grouped_loadings)
```

This test suggests that we reject the null and the non-invariant model is better. 
