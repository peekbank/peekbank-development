---
title: "Supplemental Information: Continuous developmental changes in word recognition support language learning across early childhood"
output: pdf_document
header-includes:
  - \usepackage{threeparttablex}
  - \usepackage{booktabs}
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE)
library(here)
source(here("helper","common.R"))
```

```{r, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE}
knitr::purl(here("paper","paper.Rmd"), documentation = 0)
suppressMessages(source(here("paper.R")))
```

# Dataset Description

Figure \ref{fig:dataset} gives the age distribution for each paper within our full dataset. 

```{r dataset, fig.env="figure*"}
ggplot(d_sub, aes(x = age))+ 
  geom_histogram(binwidth = 1) + 
  facet_wrap(~dataset_name, scale = "free_y")
```

Figure \ref{fig:longitudinal_admins} shows the distribution of measurement intervals for longitudinal studies within the dataset.

```{r longitudinal_admins, fig.cap="Number of administrations for datasets with repeated measurements."}

d_sub_long <- d_sub |>
  filter(subject_id %in% longitudinal$subject_id) |>
  group_by(subject_id) |>
  arrange(age) |>
  mutate(admin_num = 1:n(), 
         delta_t = age - age[1])

ggplot(d_sub_long, 
       aes(x = delta_t, fill = dataset_name)) +
  geom_histogram(binwidth = 1) + 
  # facet_wrap(~dataset_name) + 
  xlab("Age since first test")
```
# Test-Retest Reliability



```{r reliability}
MONTH_CUTOFF <- 3

longitudinal <- d_sub |>
  group_by(dataset_name, subject_id) |>
  count() |>
  filter(n > 1)

d_long <- d_sub |>
  filter(subject_id %in% longitudinal$subject_id) |>
  group_by(subject_id) |>
  arrange(age) |>
  mutate(admin_num = 1:n(), 
         time_since_t0 = age - age[1],
         delta_t = c(0, diff(age)))


d_reliability <- d_long |>
  filter(time_since_t0 <= MONTH_CUTOFF, 
         admin_num <= 2) |>
  pivot_wider(id_cols = c("subject_id","dataset_name"), 
              names_from = "admin_num",
              values_from = c("log_rt", "log_rt_var", "long_window_accuracy", 
                              "long_window_acc_var", "prod", "comp"))

dataset_reliabilities <- d_reliability |>
  group_by(dataset_name) |>
  summarise(rt = cor(log_rt_1, log_rt_2, use = "pairwise.complete.obs"),
            acc = cor(long_window_accuracy_1, 
                          long_window_accuracy_2, use = "pairwise.complete.obs")) 


# note this is not correct for CDIs because some CDIs can get grouped with two admins, inflating correlations
global_reliabilities <- d_reliability |>
  ungroup() |>
  summarise(rt = cor(log_rt_1, log_rt_2, use = "pairwise.complete.obs"),
            acc = cor(long_window_accuracy_1, 
                          long_window_accuracy_2, use = "pairwise.complete.obs"))
```


We examined test-retest reliability for our primary variables of interest by calculating Pearson correlations between pairs of administrations given no more than three months apart. Test-retest correlations were significant but relatively modest ($\rho_{acc} = `r round(global_reliabilities[2], 3)`$, $\rho_{rt} =  `r round(global_reliabilities[1],3)`$). These reliabilities were biased downwards by three factors, however. First, longitudinal assessments sometimes use variable items between testing sessions, leading to item-related variance in measurement. Second, even three months can lead to substantial change in some children's language abilities, thus correlations are attenuated by true change as well as measurement error. Third, longitudinal data in the dataset come primarily from the youngest children and hence are likely to show overall higher measurement error due to variability in children's behavior and an overall lower number of trials. 

# Correlations

Table \ref{tab:corrs} shows pairwise correlations between the primary variables of interest in the dataset.

```{r corrs, results = "asis"}

cor(select(d_sub, age, log_age, rt, log_rt, 
           long_window_accuracy, short_window_accuracy, prod, comp), 
    use = "pairwise.complete.obs") |>
  round(2) |>
  papaja::apa_table()

```

# Functional Form Model Comparison

Table \ref{tab:accmods} shows model comparison measures between for different models of the functional form for accuracy and Table \ref{tab:rtmods} shows the same for reaction time. Note that all of these comparisons With our current dataset we cannot specifically determine whether within-individual patterns of change conform to linear, power law, or exponential developmental patterns [@heathcote2000].


\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\begin{tabular}{lllllllll}
\toprule
n. obs & \multicolumn{1}{c}{sigma} & \multicolumn{1}{c}{logLik} & \multicolumn{1}{c}{AIC} & \multicolumn{1}{c}{BIC} & \multicolumn{1}{c}{REMLcrit} & \multicolumn{1}{c}{df.residual} & \multicolumn{1}{c}{model} & \multicolumn{1}{c}{r2}\\
\midrule
48354 & 0.28 & -8,338.83 & 16,691.65 & 16,753.16 & 16,677.65 & 48347 & lwa\_lin & 0.14\\
48354 & 0.28 & -8,305.39 & 16,624.78 & 16,686.28 & 16,610.78 & 48347 & lwa\_log & 0.12\\
50244 & 0.31 & -12,486.60 & 24,987.21 & 25,048.98 & 24,973.21 & 50237 & swa\_lin & 0.09\\
50244 & 0.31 & -12,458.77 & 24,931.54 & 24,993.31 & 24,917.54 & 50237 & swa\_log & 0.08\\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{center}
\end{table}

\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\begin{tabular}{lllllllll}
\toprule
n. obs & \multicolumn{1}{c}{sigma} & \multicolumn{1}{c}{logLik} & \multicolumn{1}{c}{AIC} & \multicolumn{1}{c}{BIC} & \multicolumn{1}{c}{REMLcrit} & \multicolumn{1}{c}{df.residual} & \multicolumn{1}{c}{model} & \multicolumn{1}{c}{r2}\\
\midrule
18940 & 0.45 & -12,414.69 & 24,843.38 & 24,898.32 & 24,829.38 & 18933 & log\_lin & 0.21\\
18940 & 0.45 & -12,393.27 & 24,800.53 & 24,855.48 & 24,786.53 & 18933 & log\_log & 0.21\\
18940 & 570.49 & -147,644.43 & 295,302.87 & 295,357.81 & 295,288.87 & 18933 & lin\_lin & 0.18\\
18940 & 570.02 & -147,622.67 & 295,259.34 & 295,314.29 & 295,245.34 & 18933 & lin\_log & 0.18\\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{center}
\end{table}


```{r accmods, results="asis", include = FALSE}
papaja::apa_table(acc_mods_lmer_summary)
```


```{r results="asis", include = FALSE}
papaja::apa_table(rt_mods_lmer_summary)
```


# Factor Analysis 

```{r}
d_sub_mat <- d_sub |>
  ungroup() |>
  select(dataset_name, rt, rt_var, long_window_accuracy, long_window_acc_var, prod, comp, age) 

d_sub_mat_s <- d_sub_mat |>
  ungroup() |>
  mutate(across(all_of(c("rt", "rt_var", "long_window_accuracy", 
                         "long_window_acc_var", "prod", "comp")), 
                       ~ age_scale(.x, age))) 
fa.parallel(select(d_sub_mat, -dataset_name, -age), fa = "fa", 
            use = "pairwise.complete.obs")
```

```{r}
fs <- fa(select(d_sub_mat, -dataset_name), nfactor = 3,
            use = "pairwise", rotate = "varimax")

as.data.frame(unclass(loadings(fs))) |> 
  kable(caption = "Factor loadings for the exploratory three factor solution using varimax rotation.", digits = 2)
```


# Alternative Factor Structures




- Alternative factor models: variances together, two factor, etc. etc. 

# Non-linear Growth Models

To test for the differentiation of vocabulary growth based on initial reaction time, we used the package `nlme` to fit a logistic growth model to the production data. This model has two parameters for the logistic curve, a scale and an intercept. Both were allowed to interact with initial reaction time. We also included random effects of logistic intercept and scale by participant and a grouping term across datasets. This model showed a significant effect of initial reaction time on the intercept of the logistic growth curve, but not on its scale (see Table \ref{tab:nlme}). 

% latex table generated in R 4.4.2 by xtable 1.8-4 package
% Thu Mar 13 13:10:30 2025
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & Value & Std Error & DF & $t$-value & $p$-value \\ 
  \hline
Growth Intercept & 2.26 & 2.70 & 1766.00 & 0.84 & 0.40 \\ 
  Growth Intercept $\times$ $t_0$ RT & 3.30 & 0.38 & 1766.00 & 8.59 & 0.00 \\ 
  Growth Scale & 2.05 & 2.60 & 1766.00 & 0.79 & 0.43 \\ 
  Growth Scale $\times$ $t_0$ RT & 0.28 & 0.38 & 1766.00 & 0.75 & 0.46 \\ 
   \hline
\end{tabular}
\caption{Fixed effects estimates from logistic growth model. \label{tab:nlme}}
\end{table}

Interpretation of growth in both this model and the linear growth model in the main text is complicated by the fact that the CDI form puts a ceiling on the total number of words that can be recorded; both the quadratic growth functions and the logistic functions come together at the form ceiling. Thus, a shift in quadratic growth in the linear model and a shift in intercept in the logistic model both point to the same overall effect, which is faster growth at the point of maximal sensitivity of the CDI. Neither model can estimate whether the overall growth trajectory is different beyond the range of the CDI. Thus, although these models might initially seem to be in conflict, we believe that they actually point to the same phenomenon, which is perhaps better described by the longitudinal SEM model reported in the main text. Children with greater skill in word recognition show an overall positive shift in the growth trajectory of vocabulary development.

```{r}
# xtable(summary(mod_nlme)$tTable)
```


# Latent Factor Growth


```{r}
fa3_model_long <- "
# measurement model
vocab_t1 =~ 1*prod_t1 + s1*comp_t1
accuracy_t1 =~ 1*acc_t1 + s2*acc_sd_t1
speed_t1 =~ 1*log_rt_t1 + s3*log_rt_sd_t1

vocab_t2 =~ 1*prod_t2 
accuracy_t2 =~ 1*acc_t2 + s2*acc_sd_t2
speed_t2 =~ 1*log_rt_t2 + s3*log_rt_sd_t2

vocab_t3 =~ 1*prod_t3 
accuracy_t3 =~ 1*acc_t3 + s2*acc_sd_t3
speed_t3 =~ 1*log_rt_t3 + s3*log_rt_sd_t3

vocab_t4 =~ 1*prod_t4 
accuracy_t4 =~ 1*acc_t4 + s2*acc_sd_t4
speed_t4 =~ 1*log_rt_t4 + s3*log_rt_sd_t4

# means for the latents
# vocab_t1 ~ 1
# vocab_t2 ~ 1
# vocab_t3 ~ 1
# vocab_t4 ~ 1
# accuracy_t1 ~ 1
# accuracy_t2 ~ 1
# accuracy_t3 ~ 1
# accuracy_t4 ~ 1
# speed_t1 ~ 1
# speed_t2 ~ 1
# speed_t3 ~ 1
# speed_t4 ~ 1

# autoregression for the latents
# vocab_t2 ~ 1 * vocab_t1
# vocab_t3 ~ 1 * vocab_t2
# vocab_t4 ~ 1 * vocab_t3
# accuracy_t2 ~ 1 * accuracy_t1
# accuracy_t3 ~ 1 * accuracy_t2
# accuracy_t4 ~ 1 * accuracy_t3
# speed_t2 ~ 1 * speed_t1
# speed_t3 ~ 1 * speed_t2
# speed_t4 ~ 1 * speed_t3

# regressions
accuracy_intercept =~ 1*accuracy_t1 + 1*accuracy_t2 + 1*accuracy_t3 + 1*accuracy_t4 
accuracy_slope =~ 1*accuracy_t1 + 2*accuracy_t2 + 3*accuracy_t3 + 4*accuracy_t4
speed_intercept =~ 1*speed_t1 + 1*speed_t2 + 1*speed_t3 + 1*speed_t4 
speed_slope =~ 1*speed_t1 + 2*speed_t2 + 3*speed_t3 + 4*speed_t4
vocab_intercept =~ 1*vocab_t1 + 1*vocab_t2 + 1*vocab_t3 + 1*vocab_t4 
vocab_slope =~ 1*vocab_t1 + 2*vocab_t2 + 3*vocab_t3 + 4*vocab_t4

# parameters for the regressions
vocab_intercept ~ 1
vocab_slope ~ 1
accuracy_intercept ~ 1
accuracy_slope ~ 1
speed_intercept ~ 1
speed_slope ~ 1

# residual variance for the latents
vocab_t1 ~~ NA*vocab_t1
vocab_t2 ~~ NA*vocab_t2
vocab_t3 ~~ NA*vocab_t3
vocab_t4 ~~ NA*vocab_t4
accuracy_t1 ~~ NA*accuracy_t1
accuracy_t2 ~~ NA*accuracy_t2
accuracy_t3 ~~ NA*accuracy_t3
accuracy_t4 ~~ NA*accuracy_t4
speed_t1 ~~ NA*speed_t1
speed_t2 ~~ NA*speed_t2
speed_t3 ~~ NA*speed_t3
speed_t4 ~~ NA*speed_t4

# residual variance for the other latents
vocab_intercept ~~ NA*vocab_intercept
vocab_slope ~~ NA*vocab_slope
accuracy_intercept ~~ NA*accuracy_intercept
accuracy_slope ~~ NA*accuracy_slope
speed_intercept ~~ NA*speed_intercept
speed_slope ~~ NA*speed_slope

# set observed intercepts to zero - these should go into the latents
prod_t1 ~ 0
prod_t2 ~ 0
prod_t3 ~ 0
prod_t4 ~ 0
comp_t1 ~ 0 
log_rt_t1 ~ 0
log_rt_t2 ~ 0
log_rt_t3 ~ 0
log_rt_t4 ~ 0
log_rt_sd_t1 ~ 0 
log_rt_sd_t2 ~ 0
log_rt_sd_t3 ~ 0 
log_rt_sd_t4 ~ 0 
acc_t1 ~ 0 
acc_t2 ~ 0 
acc_t3 ~ 0 
acc_t4 ~ 0 
acc_sd_t1 ~ 0
acc_sd_t2 ~ 0
acc_sd_t3 ~ 0
acc_sd_t4 ~ 0
"

fit3_long <- sem(fa3_model_long, d_sub_wide_fine, std.lv=TRUE, missing='fiml')

summary(fit3_long, fit.measures=TRUE, standardize=TRUE)

```


```{r}
layout <- read.csv(here("misc/layout_dt3.csv"), header = FALSE)
graph_sem(model = fit3_long, text_size = 3, layout=t(layout))

```


# Short Window Analysis

Need to redo all figures using short windows ugh. 
