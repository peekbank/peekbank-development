---
title: "Supplemental Information: Continuous developmental changes in word recognition support language learning across early childhood"
output: 
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    toc: true
header-includes:
  - \usepackage{threeparttablex}
  - \usepackage{booktabs}
  - \usepackage{placeins}
  - \renewcommand{\thefigure}{S\arabic{figure}}
  - \renewcommand{\thetable}{S\arabic{table}}
  - \usepackage{float}
  - \renewcommand{\textfraction}{0.00}
  - \renewcommand{\topfraction}{1}
  - \renewcommand{\bottomfraction}{1}
  - \renewcommand{\floatpagefraction}{1}
  - \setcounter{topnumber}{5}
  - \setcounter{bottomnumber}{5}
  - \setcounter{totalnumber}{6}  

---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE)
library(here)
source(here("helper","common.R"))
#library(kableExtra)
library(papaja)
# library(fitdistrplus)
# library(statmod)
# library(gamlss.dist)
# library(semoutput)


#set to TRUE to fit the Bayesian models
FIT_BAYES = TRUE
```

```{r, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE, include=F}
paper <- suppressMessages(knitr::purl(here("paper","paper.Rmd"), documentation = 0))
suppressMessages(source(paper))
```
\newpage
# S1. Dataset Description

Figure \ref{fig:dataset} gives the age distribution of unique participants for each separate dataset at different ages. Note that for some datasets, there are multiple administrations (i.e., experimental test sessions) for each participant.

```{r dataset, fig.cap="\\label{fig:dataset} Age distribution of unique participants for each dataset, using three-month bins.", fig.pos="h!"}

# summarize participants by age bin
d_sub_admin_by_age <- d_sub |>
  mutate(age_bin = age %/% 3 * 3+1.5) |>
  group_by(dataset_name, age_bin) |>
  summarise(
    n_subj = n_distinct(subject_id),
    n_administration = n_distinct(administration_id)) |>
  ungroup()

# ggplot(d_sub, aes(x = age))+ 
#   geom_histogram(binwidth=3) + 
#   facet_wrap(~dataset_name, scale = "free_y") +
#   xlab("Age (months)") + 
#   ylab("Number of administrations")+
#   theme(strip.text.x = element_text(size = 5))

# plot number of participants across age
ggplot(d_sub_admin_by_age, aes(x = age_bin,y=n_subj))+ 
  geom_bar(stat="identity",width=3) + 
  facet_wrap(~dataset_name, scale = "free_y") +
  xlab("Age (months)") + 
  ylab("Number of participants")+
  theme(strip.text.x = element_text(size = 5))

```

Figure \ref{fig:longitudinal_descriptives} shows the distribution of measurement intervals for longitudinal studies within the dataset.

```{r longitudinal_descriptives, fig.cap="\\label{fig:longitudinal_descriptives} Distribution of retest administrations across datasets with repeated measurements, colored by dataset. Each count indicates a retest administration (initial administrations are excluded). Administrations listed with a retest interval of 0 indicate retests within a month of the initial administration.", fig.pos="h!"}

d_sub_long <- d_sub |>
  filter(subject_id %in% longitudinal$subject_id) |>
  group_by(subject_id) |>
  arrange(age) |>
  mutate(delta_t = age - age[1])

ggplot(filter(d_sub_long, admin_num > 1), 
       aes(x = delta_t, fill = dataset_name)) +
  geom_histogram(binwidth = 1) + 
  scale_fill_solarized(name = "Dataset") +
  theme(legend.position = "bottom") + 
  # facet_wrap(~dataset_name) + 
  xlab("Months since first test administration") + 
  ylab("Number of administrations")
```

\FloatBarrier

# S2. Reaction Times

## S2.1. Reaction Time Computation

Eye-tracking data are stored in Peekbank as a time series of fixations to specific areas of interest (in particular, the target and distractor on each trial). Other fixations can be to areas not in the target or distractor as well to off-screen areas. This time series has a uniform sample rate of 25ms/sample, based on resampling of the data in Peekbank to 40 Hz during preprocessing (Zettersten et al. 2023). Reaction times are computed by filtering trials to only those on which the child is fixating the distractor at the point of disambiguation ($t=0$) and then finding those trials on which the first non-missing fixation is to the target (hence excluding trials without a shift and trials on which a shift is to an off-screen location). The reaction time is then the total time from $t=0$ to the first timestep during which the child fixates the target. Consistent with standard practice in the literature following Fernald et al. (2008), RTs that are shorter than 367 ms are excluded as they are too short to be considered a response to the stimulus.

\FloatBarrier

## S2.2. Comparison of Reaction Times for Correct and Incorrect Trials: Re-analysis of Creel (2024)

```{r, eval=F}
#load data using Creel (2024) analysis scripts
# To run this script, you need to first pull in two datasets from the Creel (2024) OSF repository (https://osf.io/wszd5/):
#allfamiliars.csv and allnovels.csv
library(osfr)
library(here)
creel_proj <- osf_retrieve_node("wszd5")
#donwload familiar trials
#TO DO: separate this out so files are downloaded only once
creel_proj %>%
  osf_ls_files(pattern = "allfamiliars.csv") %>% # Replace "data.csv" with your file's name
  osf_download(path = here("supplemental_data","creel_2024"),conflicts = "overwrite")
creel_proj %>%
  osf_ls_files(pattern = "allnovels.csv") %>% # Replace "data.csv" with your file's name
  osf_download(path = here("supplemental_data","creel_2024"),conflicts = "overwrite")

creel_proj %>%
  osf_ls_files(pattern = "Step2aCorrsNDataPrep.R") %>% # Replace "data.csv" with your file's name
  osf_download(path = here("supplemental_data","creel_2024"),conflicts = "overwrite")
```


```{r, include=F}
#run Creel prep script

#creel prep script expects files in root directory
detach("package:dplyr", character.only = TRUE)
library("dplyr", character.only = TRUE)
setwd(here("supplemental_data","creel_2024"))
source("Step2aCorrsNDataPrep.R")
setwd(here())
# run Peekbank-style rt computation over Creel data
source(here("supplemental_data","creel_2024","creel_peekbank_acc_rt_analysis.R"))



creel_correlation <- cor.test(filter(subj_rt_acc,n_correct_trials>1&n_trials>1)$mean_rt,filter(subj_rt_acc,n_correct_trials>1&n_trials>1)$mean_rt_correct,use="pairwise.complete.obs")

```

The Peekbank dataset only includes measurements of infants' looking behavior, with no measure of a final target selection. This contrasts with work in the visual-world paradigm with older children and adults, in which participants make a final explicit choice about which image matches the target label (e.g. Colby & McMurray, 2023). Having this additional response allows a clearer separation of accuracy and reaction times, because researchers can compute reaction times specifically on those trials in which participants responded correctly. This strategy helps avoid a possible mixing of reaction times for incorrect and correct responses, which might be generated by different underlying cognitive processes. A possible concern with the Peekbank datasets --- and reaction times in infant looking-while-listening studies more generally --- is that it is difficult to separate reaction times for correct vs. incorrect responses in the absence of an independent final choice response.

To address this concern, we investigated data from a recent large-scale word recognition study with toddlers in which eyetracking measures were collected together with a final pointing response (Creel, 2024). This dataset included 914 responses from children (2.5-6.5 years) completing a looking-while listening procedure in which they also were instructed to point to the target image. Using this dataset, we investigated the correlation between reaction times (following the same procedure as in our main analyses, i.e. focusing specifically on distractor-to-target shifts) computed over all trials and reaction times computed only for those trials in which children selected the correct referent. The results are shown in Figure \ref{fig:creelplot}. Reaction times (i.e., distractor to target shifts) for correct trials only were highly correlated with reaction times across all trials (`r apa_print(creel_correlation)$full_result`). This result suggests that having the ability to filter out incorrect trials has a minimal impact on reaction time computation, even in young children. While there is some uncertainty about how these results may generalize to infants in our younger age ranges (i.e., below 2.5 years of age), who struggle to provide reliable pointing responses, it seems reasonable to assume that our reaction time results would stay largely the same if it were possible to filter out trials on which infants make an incorrect mapping between the target label and the target image using an eyetracking-independent final choice response.


```{r creel-plot, fig.cap="\\label{fig:creelplot} Correlation between reaction times on all trials and reaction times on trials where the child pointed to the correct target. Data from Creel (2024).", fig.pos="h!", fig.width=8, fig.height=6}
 
ggplot(filter(subj_rt_acc,n_correct_trials>1&n_trials>1),aes(mean_rt,mean_rt_correct))+
  geom_point(aes(size=n_correct_trials),alpha=0.3,fill="black",stroke=NA)+
  geom_smooth(method="lm")+
  xlab("Mean Participant Reaction Time (ms)\nAll Trials")+
  ylab("Mean Participant Reaction Time (ms)\nTrials w/ Correct Pointing Response")+
  theme_cowplot(font_size=20)+
  #add correlation and p-value as text box
  annotate("text",x=500,y=1500,label=paste0("r = ",round(cor(filter(subj_rt_acc,n_correct_trials>1&n_trials>1)$mean_rt,filter(subj_rt_acc,n_correct_trials>1&n_trials>1)$mean_rt_correct,use="pairwise.complete.obs"),2)),size=7)+
  theme(legend.position=c(0.8,0.2))+
  scale_size_continuous(name="# Trials",breaks=c(2,5,10,15,20))
```

\FloatBarrier

# S3. Checks on Data Distributional Assumptions 

Here, we check whether the distributional forms that are assumed for the distributions of RT and accuracy are a reasonable empirical fit to the data, and compare against other commonly used distributional forms. 

We confirm that, across the age range, the choice to use a log-normal distribution for RT and a normal distribution for accuracy is justified. 

```{r}
library(fitdistrplus)

library(statmod)

library(gamlss.dist)

#note that fitdistrplus attaches MASS which also has a select function, and that's annoying

rt_fitting  <- d_sub |>
  ungroup() |>
  filter(!is.na(rt)) |>
  mutate(age_bin = cut(age,
                       breaks = c(0,12,15,18,21,24,27,30,36,60), 
                       labels = c("<12", "12-15","15-18", "18-21", 
                                  "21-24", "24-27","27-30","30-36", ">36"))) |>
  dplyr::select(age_bin, rt)


norm <- fitdist(rt_fitting$rt, distr = "norm", method = "mle")
lnorm <- fitdist(rt_fitting$rt, distr = "lnorm", method = "mle")
wald <- fitdist(rt_fitting$rt, distr = "invgauss", method = "mle", 
                start = list(mean = 1, shape = 1, dispersion = 1))
exgaus <- fitdist(rt_fitting$rt, distr = "exGAUS", method = "mle", 
                  start = list(mu = 1000, sigma = 500, nu = 1), 
                  lower = c(0, 0, 0))

norm_bic <- BIC(norm)
lnorm_bic <- BIC(lnorm)
wald_bic <- BIC(wald)
exgaus_bic <- BIC(exgaus)


rt_fits <- rt_fitting |>
  group_by(age_bin) |>
  nest(rt) |>
  mutate(
    norm = map(data, \(d) fitdist(d$rt, distr = "norm", method = "mle")),
    lnorm = map(data, \(d) fitdist(d$rt, distr = "lnorm", method = "mle")),
    wald = map(data, \(d) fitdist(d$rt, distr = "invgauss", method = "mle", 
                                  start = list(mean = 1, shape = 1, dispersion = 1))),
    exgaus = map(data, \(d) fitdist(d$rt, distr = "exGAUS", method = "mle", 
                                    start = list(mu = 1000, sigma = 500, nu = 1), 
                                    lower = c(0, 0, 0)))) |>
  dplyr::select(-data) |>
  pivot_longer(norm:exgaus, names_to = "distribution", values_to = "fit") |>
  mutate(BIC = map_dbl(fit, BIC)) |>
  group_by(age_bin) |>
  mutate(min_bic = BIC == min(BIC))
```

## S3.1. Reaction Time

The literature focuses on the use of the Exponential-Gaussian (ex-Gaussian) distribution as well as Wald, Weibull, gamma, and log normal distributions (see for example Luce, 1986; Ratcliff, 1993; Van Zandt, 2002). All of these are 2- or 3-parameter distributions meaning that there is no necessary relationship between mean and variance. 

The problem of fitting RT distributions is complex and a substantial literature exists (e.g., Ratcliff, 1979; Luce, 1986; Van Zandt, 2000; Baayen & Milin, 2010). One of the big challenges in our dataset as well as elsewhere is that distributions are conditional on factors such as participant and task, so it is challenging to draw inferences about the underlying distribution when looking at average data. 

That said, we find that overall the data are best fit by either an ex-Gaussian or a log normal distribution, again consistent with prior literature, giving us confidence in this conclusion. Across the full dataset, the BIC values for ex-Gaussian (`r round(exgaus_bic)|> as.character()`) and log normal (`r round(lnorm_bic)|> as.character()`) are quite close to one another, and better than the Wald (`r round(wald_bic)|> as.character()`) and normal (`r round(norm_bic)|> as.character()`) fits. When binned by age (Fig \ref{fig:rt-fits}), younger children seem better fit for a log normal distribution and older children seem better fit by ex-Gaussian (models with lowest BICs are shown in red since significant differences can be obscured by the large scale). Figure \ref{fig:rt-hist} shows the RT data distribution overlaid with the corresponding log-normal distributions. 

Overall, we think this result generally vitiates our decision to use log-transformed RTs as our primary dependent measure. 




```{r rt-fits, fig.cap="\\label{fig:rt-fits} Goodness of fit for different distributional models for RT, split by age.", fig.pos="h!", fig.width=8, fig.height=5}
ggplot(rt_fits, 
       aes(x = distribution, y = BIC, col = min_bic)) + 
  geom_point(stat = "identity", position = "dodge") + 
  scale_color_solarized(name = "Lowest BIC") + 
  xlab("Age bin") + 
  ylab("BIC") + 
  coord_flip() + 
  facet_wrap(~age_bin, scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r rt-hist, fig.cap="\\label{fig:rt-hist} Distribution of RT overlaid with a log normal distribution, split by age.", fig.pos="h!", fig.width=8, fig.height=5}

rt_hist_data <- d_sub |>
  mutate(
    age_bin = cut(age,
                  breaks = c(0,12,15,18,21,24,27,30,36,60), 
                  labels = c("<12", "12-15","15-18", "18-21", 
                             "21-24", "24-27","27-30","30-36", ">36"))) 

bw = .2

rt_hist_summary <- rt_hist_data |>
  group_by(age_bin) |>
  summarise(m = mean(log_rt, na.rm = TRUE), 
            sd = sd(log_rt, na.rm = TRUE), 
            n = n()) |>
  expand_grid(log_rt = seq(6,8,.01)) |>
  mutate(norm = dnorm(log_rt, mean = m, sd = sd) * n * bw)


ggplot(rt_hist_data,
       aes(x = log_rt)) + 
  geom_histogram(binwidth = bw) + 
  geom_line(data = rt_hist_summary, aes(x = log_rt, y = norm), col = "blue") +
  xlab("Log RT (ms)") + 
  ylab("Number of trials") + 
  facet_wrap(~age_bin)+ 
  theme_few()
```

## S3.2. Accuracy

```{r}

acc_fitting  <- d_sub |>
  ungroup() |>
  filter(!is.na(long_window_accuracy)) |>
  mutate(age_bin = cut(age,
                       breaks = c(0,12,15,18,21,24,27,30,36,60), 
                       labels = c("<12", "12-15","15-18", "18-21", 
                                  "21-24", "24-27","27-30","30-36", ">36"))) |>
  dplyr::select(age_bin, long_window_accuracy)


norm_acc <- fitdist(acc_fitting$long_window_accuracy, distr = "norm", method = "mle")
beta_acc <- fitdist(acc_fitting$long_window_accuracy, distr = "beta", method = "mle")

norm_acc_bic <- BIC(norm_acc)
beta_acc_bic <- BIC(beta_acc)
```

Individual trial-level accuracies are not binomial because they are an average probability of fixation over a viewing window. They are bounded at 0 and 1, but in general they tend towards the range .5 - .8 in most studies of this population. Figure \ref{fig:acc-hist} shows the data binned by age group with fitted gaussian distributions.

These distributions seem well fit by standard gaussians, but they are in principle bounded and so we asked whether this made a difference, using a Beta distribution (a two-parameter continuous distribution bounded at 0 and 1) for fitting. Surprisingly, across all data, the BIC values for the two distributions were very similar (`r round(norm_acc_bic) |> as.character()` for normal versus `r round(beta_acc_bic)` for Beta), though the normal distribution was slightly favored. Across age groups, there was heterogeneity with some groups better fit by a gaussian and others better fit by a Beta (Figure \ref{fig:acc-fits}).

Again, we feel that this result generally vitiates our approach of modeling accuracies via standard linear mixed-effects models: their distributional form is quite close to normal. 

```{r acc-hist, fig.cap="\\label{fig:acc-hist} Goodness of fit for different distributional models of accuracy, split by age.", fig.pos="h!", fig.width=8, fig.height=5}
acc_hist_data <- d_sub |>
  mutate(
    age_bin = cut(age,
                  breaks = c(0,12,15,18,21,24,27,30,36,60), 
                  labels = c("<12", "12-15","15-18", "18-21", 
                             "21-24", "24-27","27-30","30-36", ">36"))) 

bw = .025

acc_hist_summary <- acc_hist_data |>
  group_by(age_bin) |>
  summarise(m = mean(long_window_accuracy, na.rm = TRUE),
            sd = sd(long_window_accuracy, na.rm = TRUE),
            n = n()) |>
  expand_grid(acc = seq(0,1,.01)) |>
  mutate(norm = dnorm(acc, mean = m, sd = sd) * n * bw)


ggplot(acc_hist_data,
       aes(x = long_window_accuracy)) + 
  geom_histogram(binwidth = bw) + 
  geom_line(data = acc_hist_summary, aes(x = acc, y = norm), col = "blue") +
  xlab("Long Window Accuracy") + 
  ylab("Number of trials") + 
  facet_wrap(~age_bin) + 
  theme_few()
```



```{r acc-fits, fig.cap="\\label{fig:acc-fits} Distribution of accuracies overlaid with normal distribution, split by age.", fig.pos="h!", fig.width=8, fig.height=5}
acc_fits <- acc_fitting |>
  group_by(age_bin) |>
  nest(long_window_accuracy) |>
  mutate(
    norm = map(data, \(d) fitdist(d$long_window_accuracy, distr = "norm", method = "mle")),
    beta = map(data, \(d) fitdist(d$long_window_accuracy, distr = "beta", method = "mle"))) |>
  dplyr::select(-data) |>
  pivot_longer(norm:beta, names_to = "distribution", values_to = "fit") |>
  mutate(BIC = map_dbl(fit, BIC)) |>
  group_by(age_bin) |>
  mutate(min_bic = BIC == min(BIC))

ggplot(acc_fits, 
       aes(x = distribution, y = BIC, col = min_bic)) + 
  geom_point(stat = "identity", position = "dodge") + 
  scale_color_solarized(name = "Lowest BIC") + 
  xlab("Age bin") + 
  ylab("BIC") + 
  coord_flip() + 
  facet_wrap(~age_bin, scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

\FloatBarrier

# S4. Test-Retest Reliability

```{r reliability}
MONTH_CUTOFF <- 3

longitudinal <- d_sub |>
  group_by(dataset_name, subject_id) |>
  count() |>
  filter(n > 1)

d_long <- d_sub |>
  filter(subject_id %in% longitudinal$subject_id) |>
  group_by(subject_id) |>
  arrange(age) |>
  mutate(admin_num = 1:n(), 
         time_since_t0 = age - age[1],
         delta_t = c(0, diff(age)))


d_reliability <- d_long |>
  filter(time_since_t0 <= MONTH_CUTOFF, 
         admin_num <= 2) |>
  pivot_wider(id_cols = c("subject_id","dataset_name"), 
              names_from = "admin_num",
              values_from = c("log_rt", "long_window_accuracy", 
                              "short_window_accuracy"))

dataset_reliabilities <- d_reliability |>
  group_by(dataset_name) |>
  summarise(rt = cor(log_rt_1, log_rt_2, use = "pairwise.complete.obs"),
            acc_long = cor(long_window_accuracy_1, 
                           long_window_accuracy_2, use = "pairwise.complete.obs"),
            acc_short = cor(short_window_accuracy_1, 
                            short_window_accuracy_2, use = "pairwise.complete.obs")) 

global_reliabilities <- d_reliability |>
  ungroup() |>
  summarise(rt = cor(log_rt_1, log_rt_2, use = "pairwise.complete.obs"),
            acc_long = cor(long_window_accuracy_1, 
                           long_window_accuracy_2, use = "pairwise.complete.obs"),
            acc_short = cor(short_window_accuracy_1, 
                            short_window_accuracy_2, use = "pairwise.complete.obs"))
```

We examined test-retest reliability for our primary variables of interest by calculating Pearson correlations between pairs of administrations given no more than three months apart. Test-retest correlations were significant but relatively modest: $\rho_{long window acc} = `r round(global_reliabilities[2], 3)`$, $\rho_{short window acc} = `r round(global_reliabilities[3], 3)`$, $\rho_{rt} =  `r round(global_reliabilities[1],3)`$. These reliabilities were biased downwards by three factors, however. First, longitudinal assessments sometimes use variable items between testing sessions, leading to item-related variance in measurement. Second, even three months can lead to substantial change in some children's language abilities, thus correlations are attenuated by true change as well as measurement error. Third, longitudinal data in the dataset come primarily from the youngest children and hence are likely to show overall higher measurement error due to variability in children's behavior and an overall lower number of trials.

\FloatBarrier
# S5. Pairwise Correlations of Main Measures

Table \ref{tab:corrs} shows pairwise correlations between the primary variables of interest in the dataset.

```{=tex}
\begin{table}[H]
\caption{Pairwise correlations between primary variables of interest. \label{tab:corrs}}
\begin{center}
\begin{threeparttable}
\begin{tabular}{llllllllll}
\toprule
& \multicolumn{1}{l}{age} & \multicolumn{1}{l}{log age} & \multicolumn{1}{l}{rt} & \multicolumn{1}{l}{log rt} & \multicolumn{1}{l}{long acc} & \multicolumn{1}{l}{short acc } & \multicolumn{1}{l}{prod} & \multicolumn{1}{l}{comp}\\
\midrule
age &  1.00 &  &  &  &  &  &  & \\
log age  & 0.98 & 1.00 &  &  &  &  &  &\\
rt  & -0.33 & -0.35 & 1.00 &  &  &  &  & \\
log rt  & -0.34 & -0.36 & 0.96 & 1.00 &  &  &  & \\
long window accuracy  & 0.44 & 0.48 & -0.48 & -0.46 & 1.00 &  &  & \\
short window accuracy & 0.38 & 0.43 & -0.62 & -0.61 & 0.82 & 1.00 &  & \\
production vocabulary & 0.72 & 0.70 & -0.31 & -0.33 & 0.51 & 0.45 & 1.00 & \\
comprehension vocabulary & 0.42 & 0.42 & -0.25 & -0.24 & 0.24 & 0.24 & 0.59 & 1.00\\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{center}

\end{table}
```

```{r corrs, results = "asis", include = FALSE, eval=F}

cor(dplyr::select(d_sub, age, log_age, rt, log_rt, 
           long_window_accuracy, short_window_accuracy, prod, comp), 
    use = "pairwise.complete.obs") |>
  round(2) |> 
  papaja::apa_table()

```

\FloatBarrier

# S6. Functional Form Model Comparison

Table \ref{tab:accmods} shows model comparison measures for different models of the functional form of the relationship between accuracy and age and Table \ref{tab:rtmods} shows the same for reaction time. Age gradients are estimated substantially better with long window accuracies. Note that there are a greater number of observations for short window accuracies due to less missing data. We speculate that, on average, more participants looked away from the screen towards the end of trials, leading to a greater number of exclusions of long window trials based on the 50% criterion. Note that the total percentage of trials excluded is still small for both measures: `r round(mean(is.na(d_trial$long_window_accuracy)), 3)*100`% for long window accuracy and `r round(mean(is.na(d_trial$short_window_accuracy)), 3)*100`% for short window accuracy.

```{r}
acc_mods_lmer_summary |>
  mutate(BIC=round(BIC),
         AIC=round(AIC),
         logLik=round(logLik),
                  REMLcrit=round(REMLcrit),
         model=str_replace(model, "lwa", "Long window") |> str_replace("swa", "Short window") |> str_replace("lin", "linear age") |> str_replace("log", "log age") |> str_replace("_", ", ")
         ) |>rename(`n. obs`=nobs) |>  dplyr::select(model, everything()) |> 
kable(digits=3, caption="Model comparison metrics for different functional forms of the relationship between accuracy and age. \\label{tab:accmods}", latex_options = "HOLD_position")
```

```{r}
rt_mods_lmer_summary |>
  mutate(BIC=round(BIC),
         AIC=round(AIC),
         logLik=round(logLik),
         REMLcrit=round(REMLcrit),
         model=str_replace(model, "log_", "Log RT, ") |> str_replace("lin_", "Linear RT, ") |> str_replace("lin", "linear age") |> str_replace("log", "log age") |> str_replace("_", ", ")
         ) |> rename(`n.  obs`=nobs) |> dplyr::select(model, everything()) |> 
kable(digits=3, caption="Model comparison metrics for different functional forms of the relationship between RT and age. \\label{tab:rtmods}", format="latex", booktabs=T, latex_options = "HOLD_position")
```

\FloatBarrier

# S7. Power Law Fits

In the literature on the "law of practice", although the log-log relationship we observed is commonly present in the aggregate across individuals, the situation is substantially more complex when relationships are measured within individuals. The best fitting curves for individuals are often exponentials or delayed exponentials (Evans et al., 2018; Heathcote, Brown, & Mewhort, 2000). 

With our current dataset, we unfortunately cannot specifically determine whether within-individual patterns of change conform to linear, power law, or exponential developmental patterns, because we have insufficient data about individuals' improvement across time. Thus, our current results apply to the form of the age gradient as opposed to the form of any individual's pattern of developmental change.

We believe that, unlike the skills being studied in the prior adult literature (e.g., Anderson, 1982; Heathcote, Brown, & Mewhort, 2000; Logan, 1988), language processing is being learned over the course of a child’s lifetime. Thus, we do not expect to see within-paradigm changes in learning in what is a narrow period of time compared to the duration over which language processing skills are refined. 

Nevertheless, here we test for other forms of the aggregate relationship between age and reaction time. In particular, we consider 1) a log~log relationship between RT and age (presented in the main text), 2) using both a log age and a linear age to predict log RT, 3) a quadratic relationship between age and RT, and 4) a cubic relationship between age and RT. As shown in Table \ref{tab:age-rt}, the model with a linear age term in addition to a log age term has the best fit, although the linear age term coefficient is only marginally significant (coefficients in Table \ref{tab:loglin}). 

These models reveal a small but significant additional linear age term over and above log age, but – because individual participant-level fits are not possible – this term can’t really be used to weigh in on the debate about the precise nature of the learning pattern. 


```{r rt_more_mods}
rt_more_mods_lmer <- 
  list(log_log = lmer(log_rt ~ log_age_s + 
                        (log_age_s | dataset_name) + (1 | subject_id), 
                      data = d_trial),
       log_log_plus = lmer(log_rt ~ log_age_s + age_s +  
                             (log_age_s + age_s | dataset_name) + (1 | subject_id), 
                           data = d_trial), 
       lin_poly2 = lmer(rt ~ poly(age_s, 2) +  
                        (age_s | dataset_name) + (1 | subject_id), 
                      data = d_trial),
       lin_poly3 = lmer(rt ~ poly(age_s, 3) +  
                        (age_s | dataset_name) + (1 | subject_id), 
                      data = d_trial))


 map_df(rt_more_mods_lmer, ~broom.mixed::glance(.x)) |>
  mutate(model = names(rt_more_mods_lmer), 
         model = case_when(
           model=="log_log" ~ "Log RT, log age",
           model=="log_log_plus"~ "Log RT, log+linear age",
           model=="lin_poly2" ~ "Linear RT, poly(age,2)",
           model=="lin_poly3" ~ "Linear RT, poly(age,3)"
         ),
         r2 = map_dbl(rt_more_mods_lmer, ~performance::r2_nakagawa(.x)$R2_conditional)) |> mutate(AIC=round(AIC), BIC=round(BIC), logLik=round(logLik),          REMLcrit=round(REMLcrit),
) |> dplyr::select(model, everything()) |>  rename(`n. obs`=nobs) |> kable(digits=3, caption="Goodness of fit comparison between different models of the relationship between age and RT. \\label{tab:age-rt}", padding=0, latex_options = "HOLD_position")
```

```{r}
 log_log_plus = lmer(log_rt ~ log_age_s + age_s +  
                             (log_age_s + age_s | dataset_name) + (1 | subject_id), 
                           data = d_trial)

summary(log_log_plus)$coef |> as_tibble(rownames="Term") |> 
  mutate(Term=case_when(
  Term=="(Intercept)"~"Intercept",
  Term=="log_age_s"~ "log age",
  Term=="age_s"~"linear age",)) |> rename(`$p$-value`=`Pr(>|t|)`)|> kable(digits=3, caption="Fixed effects coefficients for a model predicting log RT from both log age and linear age. \\label{tab:loglin}", format="latex", escape=F, booktabs=T, latex_options = "HOLD_position") |>   kableExtra::kable_styling(full_width = F)
```


\FloatBarrier

# S8. Mixed-effects model specifications

Here we provide specifications for the lmer mixed-effects models used in the main text. These models are used to estimate the relationship between age and the primary variables of interest, controlling for dataset and subject-level variability.

For accuracy, 4 models were run, crossing long and short windows as the dependent variable with age or log age as the predictor. 

\begin{verbatim}
long_window_accuracy ~ age_s + (age_s | dataset_name) + (1 | subject_id)
long_window_accuracy ~ log_age_s + (log_age_s | dataset_name) + (1 | subject_id)
short_window_accuracy ~ age_s  + (age_s | dataset_name) + (1 | subject_id)
short_window_accuracy ~ log_age_s + (log_age_s | dataset_name) + (1 | subject_id)
\end{verbatim}

For reaction time, 4 models were run, crossing rt and log rt as the dependent variable  with age or log age as the predictor. 

\begin{verbatim}
log_rt ~ age_s + (age_s | dataset_name) + (1 | subject_id)
log_rt ~ log_age_s + (log_age_s | dataset_name) + (1 | subject_id)
rt ~ age_s + (age_s | dataset_name) + (1 | subject_id)
rt ~ log_age_s + (log_age_s | dataset_name) + (1 | subject_id)
\end{verbatim}

To look at the relationship between variance in the accuracy and reaction time measures and children's age, we ran two models. 

\begin{verbatim}
long_window_acc_var ~ log_age_s + (log_age_s | dataset_name) +  (1 | subject_id)
log_rt_var ~ log_age_s + (log_age_s | dataset_name) + (1 | subject_id)

\end{verbatim}

In the growth curve analysis, fit a mixed-effects model predicting growth in vocabulary as a quadratic function of age, RT at
study initiation (t0), and their interaction, using the formula below

\begin{verbatim}
prod ~ poly(age_15,2) *rt_t0 + (age  | subject_id) + (1 | dataset_name)
\end{verbatim}
          
\FloatBarrier

# S9. Factor Analysis

Figure 3 shows the result of a parallel analysis supporting the presence of three factors in the exploratory factor analysis. Table \ref{tab:loadings} shows the factor loadings for the exploratory three-factor solution using varimax rotation. The first factor is primarily driven by vocabulary measures, the second by reaction time, and the third by accuracy measures.

```{r parallel, fig.cap="Parallel analysis scree plot showing the eigenvalues for each factor, for actual, simulated, and resampled data."}
d_sub_mat <- d_sub |>
  ungroup() |>
  dplyr::select(dataset_name, rt, rt_var, long_window_accuracy, long_window_acc_var, prod, comp, age) 

d_sub_mat_s <- d_sub_mat |>
  ungroup() |>
  mutate(across(all_of(c("rt", "rt_var", "long_window_accuracy", 
                         "long_window_acc_var", "prod", "comp")), 
                ~ age_scale(.x, age))) 
outputs <- capture.output(fa.parallel(dplyr::select(d_sub_mat, -dataset_name, -age), fa = "fa", 
                                      use = "pairwise.complete.obs"))
```

```{r eval=FALSE}
fs <- fa(dplyr::select(d_sub_mat, -dataset_name), nfactor = 3,
         use = "pairwise", rotate = "varimax")

as.data.frame(unclass(loadings(fs))) |> 
  xtable(caption = "Factor loadings for the exploratory three factor solution using varimax rotation.", digits = 2)
```

<!-- % latex table generated in R 4.4.2 by xtable 1.8-4 package -->

<!-- % Mon Apr 14 12:25:14 2025 -->

```{=tex}
\begin{table}[H]
\centering
\begin{tabular}{rrrr}
\hline
& F1 & F2 & F3 \\ 
\hline
RT & -0.19 & 0.81 & -0.30 \\ 
RT var & -0.10 & 0.81 & -0.22 \\ 
long window accuracy & 0.33 & -0.31 & 0.55 \\ 
long window accuracy var & -0.10 & 0.26 & -0.65 \\ 
production vocabulary & 0.95 & -0.04 & 0.30 \\ 
comprehension vocabulary & 0.61 & -0.16 & -0.01 \\ 
age & 0.63 & -0.14 & 0.37 \\ 
\hline
\end{tabular}
\caption{\label{tab:loadings}Factor loadings for the exploratory three factor solution using varimax rotation.} 
\end{table}
```

The confirmatory factor analysis of this three-factor solution was fit using the following specification

\begin{verbatim}
vocab =~ prod + comp
accuracy =~ long_window_accuracy + long_window_acc_var
speed =~ log_rt + log_rt_var
\end{verbatim}

The confirmatory factor analysis of the three-factor solution  with a relation to age was fit using the following specification

\begin{verbatim}
vocab =~ prod + comp
accuracy =~ acc + acc_sd
speed =~ log_rt + log_rt_sd

vocab ~ log_age
accuracy ~ log_age
speed ~ log_age
\end{verbatim}

The SEM with a linear growth curve used the following specification

\begin{verbatim}
accuracy_intercept =~ 1*acc_t0 + 1*acc_t1 + 1*acc_t2 + 1*acc_t3 + 1*acc_t4
accuracy_slope =~ 1*acc_t0 + 2*acc_t1 + 3*acc_t2 + 4*acc_t3 + 5*acc_t4
speed_intercept =~ 1*log_rt_t0 + 1*log_rt_t1 + 1*log_rt_t2 + 1*log_rt_t3 + 1*log_rt_t4

speed_slope =~ 1*log_rt_t0 + 2*log_rt_t1 + 3*log_rt_t2 + 4*log_rt_t3 + 5*log_rt_t4
vocab_intercept =~ 1*prod_t0 + 1*prod_t1 + 1*prod_t2 + 1*prod_t3 + 1*prod_t4
vocab_slope =~ 1*prod_t0 + 2*prod_t1 + 3*prod_t2 + 4*prod_t3 + 5*prod_t4

accuracy_intercept ~~ NA*accuracy_intercept
accuracy_slope ~~ NA*accuracy_slope
speed_intercept ~~ NA*speed_intercept

speed_slope ~~ NA*speed_slope
vocab_intercept ~~ NA*vocab_intercept
vocab_slope ~~ NA*vocab_slope
\end{verbatim}

\FloatBarrier

# S10. Factor Analysis on First Administrations

As a robustness check, we tested our best factor analytic models using only cross-sectional data (filtering to the first test session in longitudinal datasets; N=1963 instead of N=3553). A comparison of all 4 models is shown in Table \ref{tab:factor-first}. For the three-factor CFA, the first administration model shows increased CFI (.992 instead of .972) and decreased RMSEA (.030 instead of .065). The same is true for the age-regressed three-factor CFA, which shows very good statistics on both first administrations and longitudinal data (CFI = .999 and .991, respectively and RMSEA = .009 and .037 respectively). 

```{r}
d_sub_mat_firstadmin <- d_sub_firstadmin |>
  ungroup() |>
  dplyr::select(dataset_name, log_rt, log_rt_var, long_window_accuracy, long_window_acc_var, prod, comp, age, log_age) 


d_sub_mat_firstadmin_s <- d_sub_mat_firstadmin |>
  ungroup() |>
  mutate(across(all_of(c("log_rt", "log_rt_var", "long_window_accuracy", 
                         "long_window_acc_var", "prod", "comp")), 
                ~ age_scale(.x, age))) 
```

```{r}
fit3_firstadmin <- cfa(fa3_model, d_sub_mat_firstadmin_s, 
                       std.lv=TRUE, missing='fiml')

fit_stats_firstadmin <- suppressMessages(summary(fit3_firstadmin, 
                                                 fit.measures=TRUE, 
                                                 standardize=TRUE))
```

```{r}
d_sub_mat_firstadmin_s_renamed <- d_sub_mat_firstadmin_s |>
  rename(acc = long_window_accuracy, 
         acc_sd = long_window_acc_var, 
         log_rt_sd = log_rt_var)

fit3_age_firstadmin <- cfa(fa3_age_model, 
                           d_sub_mat_firstadmin_s_renamed, 
                           std.lv=TRUE, missing='fiml')

fit3_age_summary_firstadmin <- 
  suppressMessages(summary(fit3_age_firstadmin, 
                           fit.measures=TRUE, 
                           standardize=TRUE))
```

```{r}
#remotes::install_github("dr-JT/semoutput")
library(semoutput)

output <- sem_fitmeasures(fit3, print=F) |> as_tibble() |> mutate(Model="No Age, longitudinal") |>
  bind_rows(sem_fitmeasures(fit3_firstadmin, print=F) |> as_tibble() |> mutate(Model="No Age, first admin")) |> 
  bind_rows(sem_fitmeasures(fit3_age, print=F) |> as_tibble() |> mutate(Model="Age, longitudinal")) |> 
  bind_rows(sem_fitmeasures(fit3_age_firstadmin, print=F) |> as_tibble() |> mutate(Model="Age, first admin")) |>
  dplyr::select(Model, everything()) |> mutate(AIC=round(AIC), BIC=round(BIC)) |> rename(`RMSEA$_{Lower}$`=`RMSEA_Lower`, `RMSEA$_{Upper}$`=`RMSEA_Upper`)

kable(output, digits=3, caption="Comparison of confirmatory factor analysis models on longitudinal data or first administrations only. \\label{tab:factor-first}", padding=0)
```

\FloatBarrier
# S11. Alternative Factor Structures

In this section, we provide comparisons between the three-factor model we report in the main text and several alternative models, including:

-   a one-factor model;
-   a two-factor model with vocabulary separated from speed and accuracy;
-   a two-factor model with speed separated from accuracy and vocabulary; and
-   a two-factor model with variability terms separated from speed, accuracy, and vocabulary.

\noindent Table \ref{tab:modcomp} shows the result of these comparisons. The three-factor model shows the lowest AIC and BIC, as well as being significantly better fitting than the next-best model.

```{r, eval=FALSE}
fa1_model <-  "
F1  =~ log_rt + log_rt_var + long_window_accuracy + long_window_acc_var + prod + comp"
fa1_fit <- cfa(fa1_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

fa2_model <-  "
processing =~ log_rt + log_rt_var + long_window_accuracy + long_window_acc_var
vocabulary =~ prod + comp"
fa2_fit <- cfa(fa2_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

fa2b_model <-  "
accuracy =~ long_window_accuracy + long_window_acc_var + prod + comp
speed =~ log_rt + log_rt_var"
fa2b_fit <- cfa(fa2b_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

fa2v_model <-  "
language =~ log_rt + long_window_accuracy + prod + comp
variability =~  long_window_acc_var + log_rt_var"
fa2v_fit <- cfa(fa2v_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

fa3_model <-  "
speed =~ log_rt  + log_rt_var
accuracy =~ long_window_accuracy + long_window_acc_var 
vocab =~ prod + comp"
fa3_fit <- cfa(fa3_model, d_sub_mat_s, std.lv=TRUE, missing='fiml')

xtable(anova(fa1_fit, fa2_fit, fa2b_fit, fa2v_fit, fa3_fit), digits = 2)
```

<!-- xtable 1.8-4 package -->

<!-- % Fri Apr  4 15:15:52 2025 -->

```{=tex}
\begin{table}[ht]
\centering
\caption{Model comparison for alternative factor structures. $p$-values show differences between adjacent models; no $p$-values are shown for comparisons between non-nested models. \label{tab:modcomp}}
\begin{tabular}{lrrrrrrrr}
\hline
& Df & AIC & BIC & Chisq & Chisq diff & RMSEA & Df diff & Pr($>$Chisq) \\ 
\hline
Three-factor  & 6 & 46346.69 & 46475.43 & 91.96 &  &  &  &  \\ 
Two-factor (vocab)  & 8 & 46397.74 & 46514.22 & 147.01 & 55.05 & 0.09 & 2 & $<$ 0.0001 \\ 
Two-factor (speed) & 8 & 46486.96 & 46603.44 & 236.23 & 89.22 & 0.00 & 0 &  \\ 
Two-factor (variability) & 8 & 46536.10 & 46652.58 & 285.37 & 49.14 & 0 & 0 &  \\ 
One-factor & 9 & 46535.49 & 46645.84 & 286.76 & 1.39 & 0.01 & 1 & 0.24 \\ 
\hline
\end{tabular}
\end{table}
```

\FloatBarrier
# S12. Non-linear Growth Models

```{r, eval=FIT_BAYES}
library(brms)
library(tidybayes)


d_sub_prod$age_c <- d_sub_prod$age - mean(d_sub_prod$age, na.rm = TRUE)
d_sub_prod$log_rt_0_c <- d_sub_prod$log_rt_0 - mean(d_sub_prod$log_rt_0, na.rm = TRUE)

d_sub_prod$resid_log_rt_0_c <- resid(lm(log_rt_0_c ~ log(age), data = d_sub_prod))


# Define the nonlinear formula
nlform_resid <- brms::bf(
  prod ~ 1 / (1 + exp((xmid - age_c) / exp(logscale))),
  xmid ~ 1 + resid_log_rt_0_c  + (1 | dataset_name/subject_id),
  logscale ~ 1 + resid_log_rt_0_c  + (1 | dataset_name/subject_id),
  # scale ~ 1 + log_rt_0_c,
  nl = TRUE
)

# xmid ~ 1 + log_rt_0_c + (1 | dataset_name/subject_id),
# scale ~ 1 + log_rt_0_c + (1 | dataset_name/subject_id),


priors_resid <- c(
  prior(normal(0, 5), nlpar = "xmid", coef = "Intercept"),  # xmid near center of age_c
  prior(normal(1, 1), nlpar = "logscale", coef = "Intercept"), # scale > 0, mildly steep curve
  prior(normal(0, 1), nlpar = "logscale", coef = "resid_log_rt_0_c"),
  prior(exponential(1), class = "sigma"),                # residual error
  prior(normal(0, 2), nlpar = "xmid", coef = "resid_log_rt_0_c"),
  # prior(normal(0, 1), nlpar = "scale", lb = 0)
  
  # Random effects for xmid
  prior(exponential(1), class = "sd", nlpar = "xmid", group = "dataset_name"),
  prior(exponential(1), class = "sd", nlpar = "xmid", group = "dataset_name:subject_id"),
  
  # Random effects for scale
  prior(exponential(1), class = "sd", nlpar = "logscale", group = "dataset_name"),
  prior(exponential(1), class = "sd", nlpar = "logscale", group = "dataset_name:subject_id")
)
#

# Fit the model
resid_mod_brms <- brm(
  formula = nlform_resid,
  data = d_sub_prod,
  prior = priors_resid,
  family = gaussian(),
  # init = 0,  # initialize all parameters to 0 on unconstrained scale (safe for centered priors)
  chains = 4, cores = 4,
  control = list(adapt_delta = 0.95),
  # backend = "cmdstanr",  # optional but faster
  save_pars = save_pars(all = TRUE),
  sample_prior = "yes",
  file="brms2.rds"
)

summary(resid_mod_brms)$fixed |> saveRDS("brms2_coef.rds")


new_data_resid <- expand.grid(
  age_c = seq(min(d_sub_prod$age_c), max(d_sub_prod$age_c), length.out = 100),
  resid_log_rt_0_c = c(-1, 0, 1)  # e.g., low, mean, high values
)

fitted_vals <- fitted(resid_mod_brms, newdata = new_data_resid, re_formula = NA, summary = TRUE)

saveRDS(fitted_vals, "brms2_fitted.rds")
```

```{r}
readRDS("brms1_coef.rds") |> as_tibble(rownames="type") |> 
  mutate(Type=case_when(
    type=="xmid_Intercept" ~ "Constant component of growth intercept",
    type=="xmid_log_rt_0_c" ~ "Effect of $t_0$ RT on growth intercept",
    type=="logscale_Intercept" ~ "Constant compontent of growth scale",
    type=="logscale_log_rt_0_c" ~ "Effect of $t_0$ RT on growth scale"
  )) |> 
  dplyr::select(Type, Estimate, `Est.Error`, `l-95\\% CI`=`l-95% CI`, `u-95\\% CI`=`u-95% CI`) |> kable(digits=3, caption="Fixed effects estimates from logistic growth model. \\label{tab:nlme}", format="latex", escape=F, booktabs=T, latex_options = "HOLD_position") |>   kableExtra::kable_styling(full_width = F)

```

```{r}
readRDS("brms2_coef.rds") |> as_tibble(rownames="type") |> 
  mutate(Type=case_when(
    type=="xmid_Intercept" ~ "Constant component of growth intercept",
    type=="xmid_resid_log_rt_0_c" ~ "Effect of residualized $t_0$ RT on growth intercept",
    type=="logscale_Intercept" ~ "Constant component of growth scale",
    type=="logscale_resid_log_rt_0_c" ~ "Effect of residualized $t_0$ RT on growth scale"
  )) |> 
  dplyr::select(Type, Estimate, `Est.Error`, `l-95\\% CI`=`l-95% CI`, `u-95\\% CI`=`u-95% CI`) |> kable(digits=3, caption="Fixed effects estimates from logistic growth model using RT residualized on age as the predictor. \\label{tab:nlme-resid}", format="latex", escape=F, booktabs=T, latex_options = "HOLD_position") |>  kableExtra:: kable_styling(full_width = F)

```

To test for the differentiation of vocabulary growth based on initial reaction time, we used the package `brms` to fit a (Bayesian) logistic growth model to the production data. This model has two parameters for the logistic curve, a scale and an intercept. Both were allowed to interact with initial reaction time. We also included random effects of logistic intercept and scale by participant and a grouping term across datasets. This model showed a significant effect of initial reaction time on the intercept of the logistic growth curve, but not on its scale (see Table \ref{tab:nlme}).

Age and initial reaction time were both mean-centered. 

```{r nlme-residual, fig.cap="\\label{fig:nlme-resid} Growth curves from a logistic growth model showing predicted vocabulary growth for children based on their age-residualized initial reaction times. Predictions are shown for children with initial reaction times one SD faster than the mean for their age (blue), at the mean  for their age (red), and one SD slower than the mean for their age (green). Individual longitudinal trajectories are shown in light gray. Solid lines show global model estimates and colored regions indicate 95% credible intervals.", fig.env="figure", fig.height = 4}

d_sub_prod$age_c <- d_sub_prod$age - mean(d_sub_prod$age, na.rm = TRUE)
d_sub_prod$log_rt_0_c <- d_sub_prod$log_rt_0 - mean(d_sub_prod$log_rt_0, na.rm = TRUE)

d_sub_prod$resid_log_rt_0_c <- resid(lm(log_rt_0_c ~ log(age), data = d_sub_prod))

fitted_vals <- readRDS("brms2_fitted.rds")


new_data_resid <- expand.grid(
  age_c = seq(min(d_sub_prod$age_c), max(d_sub_prod$age_c), length.out = 100),
  resid_log_rt_0_c = c(-1, 0, 1)  # e.g., low, mean, high values
)

new_data_resid$fit <- fitted_vals[, "Estimate"]
new_data_resid$lower <- fitted_vals[, "Q2.5"]
new_data_resid$upper <- fitted_vals[, "Q97.5"]

ggplot(new_data_resid, aes(x = age_c, y = fit, col = as.factor(resid_log_rt_0_c))) +
 geom_line(data = d_sub_prod, aes(x = age_c, y = prod, group=subject_id), col = "black", alpha=.1) + 
  geom_line(linewidth = 2) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = as.factor(resid_log_rt_0_c), group = as.factor(resid_log_rt_0_c)), 
              alpha = 0.3) +
  labs(x = "Centered Age", y = "Predicted prod (fixed effects only)") +
  theme_minimal() + 
  scale_color_solarized(name = "residualized log RT at t0 (SD)") + 
  scale_fill_solarized(guide = FALSE)

```

The formula specification was
\begin{verbatim}
nlform <- brms::bf(
  prod ~ 1 / (1 + exp((xmid - age_c) / exp(logscale))),
  xmid ~ 1 + log_rt_0_c  + (1 | dataset_name/subject_id),
  logscale ~ 1 + log_rt_0_c  + (1 | dataset_name/subject_id),
  # scale ~ 1 + log_rt_0_c,
  nl = TRUE
)
\end{verbatim}

And the priors were

\begin{verbatim}
priors <- c(
  prior(normal(0, 5), nlpar = "xmid", coef = "Intercept"), 
  prior(normal(1, 1), nlpar = "logscale", coef = "Intercept"), 
  prior(normal(0, 1), nlpar = "logscale", coef = "log_rt_0_c"),
  prior(exponential(1), class = "sigma"),              
  prior(normal(0, 2), nlpar = "xmid", coef = "log_rt_0_c"),

  # Random effects for xmid
  prior(exponential(1), class = "sd", nlpar = "xmid", group = "dataset_name"),
  prior(exponential(1), class = "sd", nlpar = "xmid", group = "dataset_name:subject_id"),
  
  # Random effects for scale
  prior(exponential(1), class = "sd", nlpar = "logscale", group = "dataset_name"),
  prior(exponential(1), class = "sd", nlpar = "logscale", group = "dataset_name:subject_id")
)
\end{verbatim}

Age and reaction time are correlated, so to check that the effects of initial reaction time were not due to age effects, we reran the model using residualized reaction time to remove effects of age. As seen in Table \ref{tab:nlme-resid} and Figure \ref{fig:nlme-resid}, the pattern of effects is similar for residualized reaction time as for reaction time. 

Interpretation of growth in both this model and the linear growth model in the main text is complicated by the fact that the CDI form puts a ceiling on the total number of words that can be recorded; both the quadratic growth functions and the logistic functions come together at the form ceiling. Thus, a shift in quadratic growth in the linear model and a shift in intercept in the logistic model both point to the same overall effect, which is faster growth at the point of maximal sensitivity of the CDI. Neither model can estimate whether the overall growth trajectory is different beyond the range of the CDI. Thus, although these models might initially seem to be in conflict, we believe that they actually point to the same phenomenon, which is perhaps better described by the longitudinal SEM model reported in the main text. Children with greater skill in word recognition show an overall positive shift in the growth trajectory of vocabulary development.


\FloatBarrier
# S13. SEM Longitudinal Missingness

The SEM model was fit to the entire dataset, including the large mass of cross-sectional data (to anchor the estimates of t0 coefficients) and the sparse longitudinal data for each time point. We have 3%-12% of the total t0 datapoints for any given time point (see Table \ref{tab:sem-missing}), given the sparsity of longitudinal sampling (only 6/24 of the datasets are longitudinal). 

Our data are MAR (missing at random) rather than MCAR (missing completely at random). This is because their missingness is due to which dataset they are part of – if they are from a cross-sectional dataset, they are by definition missing all longitudinal observations. For our analyses to be appropriate given this structure, we have to assume that the general developmental patterns we are studying are replicated across datasets. We believe that they are, and we show this statistically using our mixed-effects and non-linear mixed-effects models, which control for dataset-related variation. We also show dataset-level effects in a number of our visualizations for this same reason. The same degree of random effect specification that we can do in the mixed-effects models is not possible in the SEM model, however, purely for technical reasons. Again, this point highlights the importance of convergence across analyses. 


```{r}
colMeans(!is.na(d_sub_wide_fine)) |> as_tibble(rownames="type") |> filter(!type%in%c("subject_id", "dataset_name")) |>  separate_wider_regex(type, c(type = ".*", "_", timepoint = ".*")) |> pivot_wider(names_from=type, values_from=value) |> arrange(timepoint) |> dplyr::select(-log_rt_sd, -acc_sd) |>
  rename("Log RT"=log_rt, Accuracy=acc, Production=prod, Comprehension=comp) |> kable(digits=3, caption="\\label{tab:sem-missing} Fraction of data present for each measure at each time point for the longitudinal SEM. ", format="latex",  booktabs = TRUE, linesep = "", latex_options = "HOLD_position") |>   kableExtra::kable_styling(full_width = F)

```
\FloatBarrier
# Additional References

Baayen, R. H., & Milin, P. (2010). Analyzing reaction times. *International Journal of Psychological Research, 3*(2), 12-28.

Creel, S. (2024). Connecting the tots: Strong looking-pointing correlations in preschoolers' word learning and implications for continuity in language development. *Child Development, 96*, 87-103.

Logan, G. D. (1988). Toward an instance theory of automatization. *Psychological Review, 95*(4), 492–527.

Luce, P. A. (1986). A computational analysis of uniqueness points in auditory word recognition. *Perception & Psychophysics, 39*(3), 155-158.

Ratcliff, R. (1979). Group reaction time distributions and an analysis of distribution statistics. *Psychological bulletin, 86*(3), 446–461.

Ratcliff, R. (1993). Methods for dealing with reaction time outliers. *Psychological Bulletin, 114*(3), 510-532.

Van Zandt, T. (2000). How to fit a response time distribution. *Psychonomic Bulletin & Review, 7*(3), 424-465.

Van Zandt, T. (2002). Analysis of response time distributions. *Stevens’ Handbook of Experimental Psychology, 4*, 461-516.
