# Step 2 looped version to extract point and look avgs and then correlate
# Sarah C Creel
# March 7, 2023
# Last edited 4/23/24 to accommodate processing familiar word trials
#
# Previously in Step 1:
# - read in buttload of data
# - dropped bad data files as indicated in records
#
# Here in Step 2:
# - extract accuracy values from ET data
# - figure out bad ET trials and subjects to drop
# - compute correlations between point acc and look proportion
# 
#  IMPORTANT: set the variable whichCorrsToGenerate to one of 3 different values (see below)
#    default is set to 1 (overall Pearson correlations, and input data to linear mixed fx model)
#
# Default look measure:
# Target look proportion minus Competitor look proportion in a window
# Can also do 
#   - proportion of total looks that are to Target (Target / (Target + Comp))
#   - elogit (without weighting)

library(dplyr)
library(lme4)
library(ggplot2) # underutilized right now
library(readr)


#setwd("~/Documents/current studies/A0 priority/EyeMagic/newRscripts")

# included this in set of possible functions
elogit <- function(proportion,trialN) {log((trialN*proportion + .5) / (trialN*(1-proportion) + .5))}  # log() defaults to ln
# I'm not currently doing weighting...4/10/23


###################################################
# Make some selections about your analysis
# I could not get readline() to run smoothly
# Oh bother
###################################################

# FIRST: What dataset do you want to read in? n = all novel learning studies, f = familiar words only, w = familiar and novel words, a = all

readthisone='w'

#  NEXT: WHICH correlations (and cleanlookacc2 etc) do you want to generate?
#  WARNING: 1=pretty quick, 2=less so, 3=if you have a lot of parameters to loop through can take hours. ##
#  1: Overall correlations (Suppl Table 1) and input data, and main linear mixed effects model
#  2: varied window lengths in Figure 4
#  3: kid age windows in suppl figs
#  4: user-defined correlations (be sure to edit script)!
###  ICC option removed, I now use splithalf library for reliability estimates

whichCorrsToGenerate=1

##################################
# Read in selected dataset
##################################
if (readthisone=="n"){
  dat=read.csv('allnovels.csv') ## allnovels is generated by a long ugly script that is omitted in shared version.
} else if (readthisone=="f"){
  dat=read.csv('allfamiliars.csv') ## allfamiliars is similarly generated by a long ugly script that is omitted in shared version. 
} else if (readthisone=="w"){
  dat=read.csv('allnovels.csv');dat2=read.csv('allfamiliars.csv');
  dat=rbind(select(dat,-Subsub),dat2) # this gives you novels and fam-word trials 
  dat= dat %>% filter(ExptSubcat %in% c("wordlearning","familiarwords"))
} else if (readthisone=="a"){
  dat=read.csv('allnovels.csv');dat2=read.csv('allfamiliars.csv');
  dat=rbind(select(dat,-Subsub),dat2) # this gives you novels and fam-word trials
} else {print("Did not recognize selection, restart.")
}

unique(dat$ExptSubcat) # check that included conditions are as intended



###############################################################
#### now assign variables to loop thru based on what was set above,
#### including REFERENCE value
#### SET YOUR VALUES TO CHECK INDIVIDUALLY HERE ('Reference')
#### Currently the 'Reference' is individual conditions w/in each expt
#### So there are more vals of Reference than total N of expts
###############################################################

if (whichCorrsToGenerate==1){
  #### this version does input to the linear mixed effects model: 200-2000 ms, subset pointing acc, dynamic looks, TminusC as look measure, pearson r
  dat$Reference=as.factor(paste(dat$ExactExptLabel,dat$ConditionClean,dat$ExptSubcat,sep="-"));reftype='CondinExp' # CONDITION WITHIN EXPERIMENT
  wintype='growing';winsize=1800;winmax=2000;acsource='subset';looksource='dynamic';lookmsr='TminusC';corris='pearson'
  filename="TminusC2002000"
} else if (whichCorrsToGenerate==2) {
  ## complicated version!! lots of different variables. 
  ## should generate 
  dat$Reference=as.factor(dat$ExptSubcat);reftype='ExperimentSubcategory' # EXPERIMENT TYPES--TALKERS, MELODIES, WORDS
  wintype= c('growing','moving','moving','moving','moving')
  winsize=c(100,100,200,400,800)
  winmax=c(3000)
  acsource=c('subset') # can also be 'totacc'
  looksource=c('dynamic') # can also be 'static'
  lookmsr=c('TminusC') # possible: 'TminusC','TdivTC', 'elogitTmC'
  corris=c('pearson') # can also be 'spearman'
  filename="windfbysubjectswithinGenExpType"
} else if (whichCorrsToGenerate==3) {  # this is for moving window kid ages
  ## extra complicated version!! you can fill your values in here. go for it.
  dat$Reference=as.factor(dat$AgeGroups);reftype='KidsAges' #### age groups of kids-- 3/4/5-year-olds
  wintype= c('growing','moving')
  winsize=c(100,100) # deliberately stupid values to make you pick your own
  winmax=c(3000)
  acsource=c('subset') 
  looksource=c('dynamic')
  lookmsr=c('TminusC')
  corris=c('pearson') #
  filename='bychildageGrowingMoving'
} else if (whichCorrsToGenerate==4) {  # this is for users to fill in but still retain 1st two analyses
  ## extra complicated version!! you can fill your values in here. go for it.
  dat$Reference=as.factor(paste(dat$ExactExptLabel,dat$Gender,sep="-"));reftype='WholeExpsXGender' #### EACH EXPERIMENT X GENDER
  wintype= c('growing','moving')
  winsize=c(100,101,102,103) # deliberately stupid values to make you pick your own
  winmax=c(2000)
  acsource=c('subset','totacc') # can also be 'totacc'
  looksource=c('dynamic','static') # can also be 'static'
  lookmsr=c('TdivTC') # possible: 'TminusC','TdivTC', 'elogitTmC'
  corris=c('spearman') # can also be 'spearman'
  filename='userdefinedwindowdataframe'}
# } else if (whichCorrsToGenerate==5) {  # to split everything into firsthalf-secondhalf # ###### THIS IS NEW AS OF 12/20/2023 #######
#   # first do condition-within-expt
#   mintrials=1; print("WARNING!!! MINTRIALS IS SET TO 1. NORMALLY IT'S SET TO 4.")
#   dat$preReference=as.factor(paste(dat$ExactExptLabel,dat$ConditionClean,sep="-"));reftype='CondinExp' # CONDITION WITHIN EXPERIMENT
#   
#   # trials are RANKED rather than just split bc sometimes a particular condition
#   #  will be missing trials (real word trials) or will go from 9-16 or will have
#   #  different trial number totals than other experiments
#  # halftype="first-second" # this does firsthalf-secondhalf of each participant
#   halftype="odds-evens" # this does (ranked) odds vs evens, more temporally interdigitated
#   
#   if (halftype=="first-second") {
#     library(plyr) # there's a dplyr way to do this but not homologous, I'm too lazy to figure out
#     dat=ddply(dat, c("preReference","Subject"), transform, ScaledTrialNum = scale(rank(Trialcount))) ## IN PLYR
#     dat$SplitHalves=if_else(dat$ScaledTrialNum<=0,"FirstHalf","SecondHalf")
#     unloadNamespace("plyr") # have to redrop plyr bc summarise() conflicts with dplyr
#     dat %>% group_by(SplitHalves) %>% summarise(min(ScaledTrialNum),max(ScaledTrialNum),mean(ScaledTrialNum),n()) # check it
#       }
#   if (halftype=="odds-evens") {
#     dat$SplitHalves=if_else(dat$Trialcount %% 2 == 1,"FirstHalf","SecondHalf")
#   }
#   
#   # check...
#   
#   # RESET Reference to be Reference_Half
#   dat$Reference=as.factor(paste(dat$preReference,dat$SplitHalves,sep="_"));reftype='CondinExp_RankedHalf' 
#     
#   wintype= c('growing')
#   winsize=c(1800) # deliberately stupid values to make you pick your own
#   winmax=c(2000)
#   acsource=c('subset') # can also be 'totacc'
#   looksource=c('dynamic') # can also be 'static'
#   lookmsr=c('TminusC') # possible: 'TminusC','TdivTC', 'elogitTmC'
#   corris=c('pearson') # can also be 'spearman'
#   filename='SplitHalves'
# }

## STANDARD VALUE OF MINTRIALS, for everything but the reliability analyses
if (whichCorrsToGenerate!=5) {  # to split everything into firsthalf-secondhalf
  mintrials=4 
}




summary(dat$Reference) # show what your Reference is
dat$TrialIDish=as.factor(paste(dat$Subject,dat$Trialcount)) # keep each single trial unique for later identifiability/dropping
totalsubs=length(unique(dat$Subject)) 
totalsubs		# 925 total unique subjects before any eye tracking eliminations


# N VS NITEMS--N = cells in looking trials after dropping bad (?) and Nitems = cells in accuracy data


# @@@@@@@@@@@@ presets @@@@@@@@@@@
# @@@@@@@@@@@@ presets @@@@@@@@@@@
# @@@@@@@@@@@@ presets @@@@@@@@@@@

vals2check=names(summary(as.factor(as.character(dat$Reference)))) # list of thingies to loop through # *$*$*$*$*$*$*$*$*
# converting to char and back to factor drops out empty cells that were filtered out. prob better way to do this.

# Things to keep track of:
# TRIALSKEPT: How many trials were kept?
# CELLSKEPT: How many data cells were kept in final dataset after dropping cells with too few trials (in each round, if multiple rounds?)
# NANTRIALSDROPT: For T/(T+C) as look measure, how many trials were dropped due to NaN values? (A lot if single trial, few if multiple trials)
# ALLPROB: the correlation computed across all data points regardless of Reference
trialskept=0; cellskept=0; nantrialsdropt=0;
allprob=0; 

# values that are fixed, for now
lowerlimit=200 # lower time window limit
# mintrials=4 # SPECIFIED ABOVE
lookaccfilter=.50  # drop out trials with MORE than LOOKACCFILTER% looks offscreen/blinking(filledblink)
stepsize=100
accmin=0 # Lowest pointing accuracy value allowed (most inclusive = 0)
accmax=1 # Highest pointing accuracy value allowed (most inclusive = 1)







analysiscounter=0
for (aa in 1:length(wintype)) {
  windowtype=wintype[aa]
  movwinsize=winsize[aa]
  
  for(bb in 1:length(winmax)) {
    upperlimit=winmax[bb]
    
    for(cc in 1:length(acsource)) {
      useacc=acsource[cc]
      
      for(dd in 1:length(looksource)) {
        lookfiltermode=looksource[dd]
        
        for(ee in 1:length(lookmsr)) {
          measuretype=lookmsr[ee]
          
          for(ff in 1:length(corris)) {
            corrmethod=corris[ff]
            
            print(paste('running',windowtype,movwinsize,upperlimit,useacc,lookfiltermode,measuretype,corrmethod))
            analysiscounter=analysiscounter+1

{ # basic setup before looping through values
  windows=((lowerlimit+movwinsize)/stepsize):(upperlimit/stepsize)*stepsize ## variableized! 3/24/23

# TOTACC: The pointing accuracy data before dropping any trials. This may not get used.
# To best match looks, use pointing accuracy only on 'good' look trials;
#    can recalc accuracy for the proportion of 'good' trials anew per time window.
acdat=filter(dat,WOT==100)  # pick single time slice to get a single data line per trial; can't be 0 b/c starts at 0
acdat$Accuracy=as.numeric(acdat$Response=="target")
totacc=summarise(group_by(ungroup(acdat),Subject,Reference,Age,AgeGroups,Gender,ExptSubcat,ExactExptLabel),Accuracy=mean(Accuracy),Nitems=n_distinct(Trialcount))
totacc %>% group_by(Nitems) %>% summarise(n()) # about 22 ppts appear to be missing a trial, so like 2.3%. Think it's an ET thing as trials show up in demodata.


# OVERALL LOOK PROPORTION CHECKING
# Can use for STATIC look filtering
totlookdat=dat %>% filter(WOT>lowerlimit,WOT<=upperlimit) %>% group_by(Subject,Reference,Age,AgeGroups,Gender,ExptSubcat,ExactExptLabel,Trialcount,TrialIDish,Tloc) %>% 
  summarise(Targ=mean(Targ),Comp=mean(Comp),BL=mean(BadLooks),Ext=mean(Extended))
## NOTE! 3/7/23. BadLooks is, at the SINGLE-SAMPLE level, either Blinking=1,Offscreen=1,or both.
## BadLooks CAN include looks that were filled in (in an attempt to correct for pointing gesture covering camera)
## It's coded as max(Offscreen,Blinking) in portToObjs Python script. That makes sense for single samples 
## even though it seems not to make sense here.
## A bad look, then, means either the eye couldn't be located, or look was not onscreen.
print(paste("Total badlook trials are",round(length(filter(totlookdat,BL>=.5)$BL)/length(totlookdat$BL),3))) # ~16% of all trials, 200-2000, are >=50% bad, if you use static look filtering

# drop bad looks:
#   drop trials with >50% looks filled (inferred from last look location) or offscreen
#     (this does potentially bias against trials where pointing was early but oh well)
#   Drop subjects with <4 trials/condition

## This is the data frame that will contain all of the look-point correlation values
windf=data.frame(TimeWin=as.numeric(),References=as.character(),CorrValue=as.numeric(),Nsubjects=as.numeric())
}

            
            
############################################################
# BIG loop to calculate accuracy-looks correlation over time
# then saves all those values to big big files
#            
# Ironically, for the simplest presets it doesn't actually
# loop, it just does one pass.

# Loop structure just allows you to do moving window, 
# growing window, and/or multiple options (see 'complicated 
# version' above)
############################################################
############################################################

{
bigloopchecker=as.numeric()

for (counter in 1:length(windows))
{

  # the loop will go through whatever windows are defined here
	if (windowtype=="growing"){
		dat2=filter(dat,WOT>lowerlimit,WOT<=(windows[counter])) # expanding window
	} 
	else if (windowtype=="moving") {
		dat2=filter(dat,WOT>windows[counter]-movwinsize,WOT<=(windows[counter])) # moving window
	}
	else if (windowtype=="shrinking") { # moving larger window...shrinks at end. Not sure if this works, try at own risk!
		dat2=filter(dat,WOT>windows[counter]-movwinsize,WOT<=(windows[counter]+movwinsize)) # moving window
	}


  # How to eliminate trials with bad looks?
  if (lookfiltermode=="dynamic") { # eliminate anew for each step in the loop
    dat3=ungroup(dat2) %>% group_by(Subject,Reference,Trialcount,TrialIDish,Response,Age,AgeGroups,Gender,ExptSubcat) %>% 
      summarise(Targ=mean(Targ),Comp=mean(Comp),ND=mean(Nodata),BL=mean(BadLooks),Ext=mean(Extended)) # bad look measure from CURRENT looks
  	datcleaned=filter(dat3,BL<=lookaccfilter) # Look filtering. Take out the trials with BL (bad looks) at or above filter
  }
  else if (lookfiltermode=="static") { # eliminate based on the original pointing 
    dat3=ungroup(dat2) %>% group_by(Subject,Reference,Trialcount,TrialIDish,Response,Age,AgeGroups,Gender,ExptSubcat) %>% 
      summarise(Targ=mean(Targ),Comp=mean(Comp),ND=mean(Nodata),Ext=mean(Extended)) # DONT include BadLooks
    dat3=left_join(dat3,select(totlookdat,TrialIDish,BL)) # for static: get bad look measure from OVERALL looks data set
    datcleaned=filter(dat3,BL<=lookaccfilter) # Look filtering
  }
  else if (lookfiltermode=="none") { # don't take out trials for bad looks
    dat3=ungroup(dat2) %>% group_by(Subject,Reference,Trialcount,TrialIDish,Response,Age,AgeGroups,Gender,ExptSubcat) %>% 
      summarise(Targ=mean(Targ),Comp=mean(Comp),ND=mean(Nodata),Ext=mean(Extended)) # no bad look removal step
    datcleaned=dat3
    trialskept[counter]=max(dim(datcleaned))/max(dim(dat3)) # % trials remaining at each iteration
  }
  else if (lookfiltermode=="reverse-dynamic") { # KEEP ONLY BAD looks; eliminate anew for each step in the loop
    dat3=ungroup(dat2) %>% group_by(Subject,Reference,Trialcount,TrialIDish,Response,Age,AgeGroups,Gender,ExptSubcat) %>% 
      summarise(Targ=mean(Targ),Comp=mean(Comp),ND=mean(Nodata),BL=mean(BadLooks),Ext=mean(Extended)) # bad look measure from CURRENT looks
    datcleaned=filter(dat3,BL>lookaccfilter) # Look filtering. Take out the trials with BL (bad looks) ABOVE filter (WEIRD! JUST TO CHECK!)
  }
  trialskept[counter]=max(dim(datcleaned))/max(dim(dat3)) # percent trials remaining at each iteration, after removing bad looks
  
  
  
  


	###########################################
	# get average pointing accuracy (if needed)
  # and average looking proportions
	#########################################

  # use the subset of 'good looks' trials to calculate pointing accuracy
	if (useacc=='subset'){ 
  	datcleaned$Accuracy=as.numeric(datcleaned$Response=="target")
  	subsetacc=ungroup(datcleaned) %>% group_by(Subject,Reference,Age,AgeGroups,Gender,ExptSubcat) %>% 
  	  summarise(Accuracy=mean(Accuracy))
	}

	# average the cleaned looking proportions ACROSS TRIALS within a subject within a condition; track total trials
	datcleanedagg= ungroup(datcleaned) %>% group_by(Subject,Reference,Age,AgeGroups,Gender,ExptSubcat) %>% 
	  summarise(Targ=mean(Targ),Comp=mean(Comp),N=n_distinct(Trialcount),Extended=mean(Ext))
	

	
	#################################
	#### Merges together pointing 
	#### accuracy and looking data 
	#### to compute relationship
	#################################
	#### (adds a conglomerator [SC's term]
	#### column called 'Cond' to both 
	#### looking and pointing data 
	####  to allow merging)
	#################################
	
	# conglomerator for looks--4/23/24 now includes ExptSubcat to disambiguate familiar words
	datcleanedagg$Cond=paste(datcleanedagg$Subject, datcleanedagg$Reference, datcleanedagg$ExptSubcat, sep="")
	
	# conglomerator for points and and join data
	if (useacc=="totacc"){ # use total accuracy, before dropping bad looks
		totacc$Cond=paste(totacc$Subject, totacc$Reference,datcleanedagg$ExptSubcat, sep="") # note ExptSubcat also 4/23/24
		cleanlookacc=dplyr::left_join(datcleanedagg,select(ungroup(totacc),Accuracy,Cond),by="Cond") #*****
	}else if (useacc=="subset"){ # use subset accuracy, after dropping bad looks
		subsetacc$Cond=paste(subsetacc$Subject, subsetacc$Reference, datcleanedagg$ExptSubcat, sep="") # note ExptSubcat also 4/23/24
		cleanlookacc=dplyr::left_join(datcleanedagg,select(ungroup(subsetacc),Accuracy,Cond),by="Cond")
	}
	#################################
	## cleanlookacc now contains good looks and whatever accuracy data
	#################################
	
	#################################
	# CALCULATE actual looking measure here.
	#################################
	### Note: 3/9/23 T/(T+C) is a problem bc many cells have no looks to either, which gives NaN.
	### so best to condense into single cell and then calc TTC, then STILL have to drop NaN trials
	### tracked in nantrialsdropt
	#################################
	
	if (measuretype=="TminusC"){ # Targ-Comp
	  cleanlookacc$Looks=cleanlookacc$Targ-cleanlookacc$Comp
	} 
	else if (measuretype=="TdivTC") { # T/(T+C)  ### THIS IS DONE OVER AGGREGATED TRIALS
	  cleanlookacc$Looks=cleanlookacc$Targ/(cleanlookacc$Targ+cleanlookacc$Comp)
	  nantrialsdropt[counter]=length(filter(cleanlookacc,is.nan(Looks)==TRUE)$Looks)/length(filter(cleanlookacc)$Looks)
	  nantrialsdropt[counter] # about 10% for whatever I'm looking at...
	  cleanlookacc=filter(cleanlookacc,is.nan(Looks)==FALSE)
	}
	else if (measuretype=="elogitTmC") { #elogit(T)-elogit(C)
	  cleanlookacc$Looks=elogit(cleanlookacc$Targ,cleanlookacc$N)-elogit(cleanlookacc$Comp,cleanlookacc$N)
	}
	
	###########################################
	# Trial minimum cleaning--at least (mintrial) trials per cell (set at 4)
	###########################################
	
	prelength=max(dim(cleanlookacc))   # @@@@@@@@@@@@@@
	cleanlookacc2=filter(cleanlookacc,N>=mintrials)
	
	# how many elements (cells, containing variable numbers of trials) dropped?
	cellskept[counter]=max(dim(cleanlookacc2))/prelength   # @@@@@@@@@@@@@@ this is like another 1-6% # for simple, keeping .97776%
	
	###########################################
	### optionally drop low (or high) pointing accuracy
	### currently set not to drop
	###########################################
		# could exclude 'flippers' who are anti-accurate, or ceiling/floor values
	# but not all flippers (<<chance) are flippers, some are answering based on conflicting cue
	cleanlookacc2=filter(cleanlookacc2,Accuracy>=accmin,Accuracy<=accmax)
	
	###########################################
	## center pointing accuracy on 0 (new range: -.5 to +.5)
	## better for calculating intercept deviation from chance
	###########################################
	cleanlookacc2$Acc50=cleanlookacc2$Accuracy-.5
	
	
	#################################
	##    NOW THE REAL FUN BEGINS!
	## Loop thru each Reference value
	## and calculate look-point correlation,
	## then add to big data frame (windf)
	#################################
	#################################
	#################################
	
	attach(cleanlookacc2)
	
	for (rose in 1:length(vals2check)){ # loop through all the 'references' (experiments, or conditions within experiment, or whatever)
  	tempcor=cor.test(Looks[Reference==vals2check[rose]], Accuracy[Reference==vals2check[rose]],method=corrmethod) # .131, p = .48  # gets hung up here because no L6-AP ???
  	tinydf=data.frame(TimeWin=windows[counter],References=vals2check[rose],CorrValue=tempcor$estimate,Nsubjects=n_distinct(Subject[Reference==vals2check[rose]]))
  	windf=rbind(windf,tinydf)
	}
	
	# Finally, computes correlation across ALL data points regardless of study...less relevant but interesting
	allcor=cor.test(Looks, Accuracy,method=corrmethod)#
	allprob[counter]=allcor$estimate
	tinydf=data.frame(TimeWin=windows[counter],References="Overall",CorrValue=allcor$estimate,Nsubjects=n_distinct(cleanlookacc2$Cond)) # CONDITION within subejct
	windf=rbind(windf,tinydf)
	
	bigloopchecker=c(bigloopchecker,1)
	detach(cleanlookacc2)
}  #### end of giant loop


###########################################
# This section saves the 'windf' data frame in R 
# but it's also still available as a variable
###########################################

windfToSave=windf %>% mutate(groupis=reftype,lowerlimit=lowerlimit,upperlimit=upperlimit,windowtype=windowtype,
                             measuretype=measuretype,corrmethod=corrmethod,mintrials=mintrials,
                             useacc=useacc,accmin=accmin,accmax=accmax,
                             lookaccfilter=lookaccfilter,lookfiltermode=lookfiltermode,
                             movwinsize=movwinsize,stepsize=stepsize,totalsubs=totalsubs,date=date())

write_csv(windfToSave,paste(filename,"csv",sep='.'),append=TRUE) # how to append


###########################################
# output to data file the trackers for % trials dropped/kept at various stages.
###########################################

### NOTE: 4/11/23: as currently coded, timewin is not represented properly
## but the time windows should increase throughout each data set. Each one will
## have extra time windows, which are left over from the previous longer-window run(s),
## like there are slots in trialskept for bins ending in 2100-3000 ms even when
## upperlimit is set back to 2000.
## Should fix by resetting trialskept,nantrialsdropt,cellskept to c().

# initial step of dropping bad looks trials--what proportion
trialskeptdf=data.frame(trialskept=trialskept,timewin=windows[counter],groupis=reftype,lowerlimit=lowerlimit,upperlimit=upperlimit,windowtype=windowtype,
                        measuretype=measuretype,corrmethod=corrmethod,mintrials=mintrials,
                        useacc=useacc,accmin=accmin,accmax=accmax,
                        lookaccfilter=lookaccfilter,lookfiltermode=lookfiltermode,
                        movwinsize=movwinsize,stepsize=stepsize,totalsubs=totalsubs,date=date())
write_csv(trialskeptdf,paste(filename,"_trialskept(nonbadlooks).csv",sep=''),append=TRUE)

# just relevant for TdivTC, how many trials dropped bc t/(t+c) was NA? happens before cellskept
if (measuretype=="TdivTC"){
  nantrialsdroptdf=data.frame(nantrialsdropt=nantrialsdropt,timewin=windows[counter],groupis=reftype,lowerlimit=lowerlimit,upperlimit=upperlimit,windowtype=windowtype,
                              measuretype=measuretype,corrmethod=corrmethod,mintrials=mintrials,
                              useacc=useacc,accmin=accmin,accmax=accmax,
                              lookaccfilter=lookaccfilter,lookfiltermode=lookfiltermode,
                              movwinsize=movwinsize,stepsize=stepsize,totalsubs=totalsubs,date=date())
  write_csv(nantrialsdroptdf,paste(filename,"_nantrialsdropt(forTdivTC).csv",sep=''),append=TRUE)
}

# second step of dropping cells with fewer than MINTRIALS trials--what proportion of remainder dropped
cellskeptdf=data.frame(cellskept=cellskept,timewin=windows[counter],groupis=reftype,lowerlimit=lowerlimit,upperlimit=upperlimit,windowtype=windowtype,
                       measuretype=measuretype,corrmethod=corrmethod,mintrials=mintrials,
                       useacc=useacc,accmin=accmin,accmax=accmax,
                       lookaccfilter=lookaccfilter,lookfiltermode=lookfiltermode,
                       movwinsize=movwinsize,stepsize=stepsize,totalsubs=totalsubs,date=date())
write_csv(cellskeptdf,paste(filename,"_cellskept(mintrialsplus).csv",sep=''),append=TRUE)

# reset trackers to 0
cellskept=0; trialskept=0; nantrialsdropt=0;
}

          }
        }
      }
    }
  }
}

print(paste(analysiscounter,'number of trials run'))



## how many subjects left after look eliminations? (can change with analysis parameters)
cleanlookacc2 %>% group_by(ExptSubcat) %>% summarise(n_distinct(Subject))
### For the REPORTED VALUES IN MANUSCRIPT
# ExptSubcat     `n_distinct(Subject)`
# <chr>                          <int>
#   1 talkerlearning                   130 (dropped 1)
# 2 vismusassoc                      239 (dropped 2)
# 3 wordlearning                     545 (dropped 8)
cleanlookacc2 %>% group_by(Gender) %>% summarise(n_distinct(Subject))
# Gender `n_distinct(Subject)`
# <chr>                  <int>
#   1 ""                         7
# 2 "???"                      1
# 3 "F"                      451
# 4 "M"                      455

